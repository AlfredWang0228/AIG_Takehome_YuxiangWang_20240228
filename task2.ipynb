{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "30a2067a-2a86-40b3-ba16-b7f9af51c5b3",
   "metadata": {},
   "source": [
    "# Task Description\n",
    "\n",
    "- **Task Requirements:**  \n",
    "  The objective of this task is to develop a Gen AI solution that can extract specific data attributes from a selected company's 10-K filings for a given year. The workflow includes:\n",
    "  - Converting the documents into chunks.\n",
    "  - Converting the chunks into embeddings.\n",
    "  - Creating a query.\n",
    "  - Designing a prompt to extract data from chunks for a specific year.\n",
    "  - Creating a validation dataset containing five true values extracted from the chunks.\n",
    "  - Demonstrating that the LLM can retrieve the correct chunks from the embedding object for the specified year.\n",
    "\n",
    "- **Dataset Description:**  \n",
    "  - The dataset consists of 10-K filings from the years 2018 to 2020.\n",
    "  - It includes all sections of the 10-K filing.\n",
    "  - The analysis will focus on one selected company’s filings from a single year.\n",
    "  - Five key data attributes will be extracted from the chosen company's annual report.\n",
    "\n",
    "- **Selection Strategy:**  \n",
    "  - A single company will be chosen for analysis.\n",
    "  - The focus will be on one year’s 10-K filing to ensure precision in retrieval.\n",
    "  - Five specific data attributes will be selected for extraction to validate the effectiveness of the LLM-driven retrieval and extraction process.\n",
    "\n",
    "> **Note:** This approach ensures that the task remains focused on validating the LLM's capability to retrieve and extract relevant data attributes from embeddings while maintaining scope control.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e5865af7-b9b6-4806-86e1-09bcb74b82d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to\n",
      "      ____              __\n",
      "     / __/__  ___ _____/ /__\n",
      "    _\\ \\/ _ \\/ _ `/ __/  '_/\n",
      "   /___/ .__/\\_,_/_/ /_/\\_\\   version 3.5.5\n",
      "      /_/\n",
      "                        \n",
      "Using Scala version 2.12.18, OpenJDK 64-Bit Server VM, 11.0.26\n",
      "Branch HEAD\n",
      "Compiled by user ubuntu on 2025-02-23T20:30:46Z\n",
      "Revision 7c29c664cdc9321205a98a14858aaf8daaa19db2\n",
      "Url https://github.com/apache/spark\n",
      "Type --help for more information.\n"
     ]
    }
   ],
   "source": [
    "!spark-submit --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aa5d5da2-cec2-4776-b99e-6f7aeb50f2f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"utils\")\n",
    "\n",
    "import re\n",
    "import math\n",
    "import json\n",
    "import datasets\n",
    "from typing import List\n",
    "from datasets import load_dataset, concatenate_datasets\n",
    "\n",
    "import ollama\n",
    "from ollama import chat\n",
    "from ollama import ChatResponse\n",
    "from pydantic import BaseModel\n",
    "from typing import Optional, Dict, Any\n",
    "\n",
    "# Spark / Spark NLP\n",
    "import sparknlp\n",
    "from sparknlp.pretrained import PretrainedPipeline\n",
    "from sparknlp.base import DocumentAssembler\n",
    "from sparknlp.annotator import E5Embeddings\n",
    "\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import (\n",
    "    col, size, udf, expr, split, rand, mean, stddev,\n",
    "    percentile_approx, when\n",
    ")\n",
    "from pyspark.sql.types import ArrayType, StringType, FloatType, DoubleType\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.linalg import Vectors, VectorUDT\n",
    "\n",
    "\n",
    "# Local imports\n",
    "from convert_dataset_to_spark import convert_dataset_to_spark_df\n",
    "from section_processing import SectionProcessor\n",
    "from plot_word_counts import plot_section_word_counts\n",
    "\n",
    "# TQDM progress\n",
    "from tqdm.auto import tqdm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64c10d11-fb23-4db7-8106-7d19162a9827",
   "metadata": {},
   "source": [
    "# Initialize & Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1f804f78-f72c-4622-846d-f52d24d898ae",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/Users/yuxiangwang/venv/lib/python3.13/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /Users/yuxiangwang/.ivy2/cache\n",
      "The jars for the packages stored in: /Users/yuxiangwang/.ivy2/jars\n",
      "com.johnsnowlabs.nlp#spark-nlp_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-f8939018-dba4-43b1-9516-96ecd616a606;1.0\n",
      "\tconfs: [default]\n",
      "\tfound com.johnsnowlabs.nlp#spark-nlp_2.12;5.5.3 in central\n",
      "\tfound com.typesafe#config;1.4.2 in central\n",
      "\tfound org.rocksdb#rocksdbjni;6.29.5 in central\n",
      "\tfound com.amazonaws#aws-java-sdk-s3;1.12.500 in central\n",
      "\tfound com.amazonaws#aws-java-sdk-kms;1.12.500 in central\n",
      "\tfound com.amazonaws#aws-java-sdk-core;1.12.500 in central\n",
      "\tfound commons-logging#commons-logging;1.1.3 in central\n",
      "\tfound commons-codec#commons-codec;1.15 in central\n",
      "\tfound org.apache.httpcomponents#httpclient;4.5.13 in central\n",
      "\tfound org.apache.httpcomponents#httpcore;4.4.13 in central\n",
      "\tfound software.amazon.ion#ion-java;1.0.2 in central\n",
      "\tfound joda-time#joda-time;2.8.1 in central\n",
      "\tfound com.amazonaws#jmespath-java;1.12.500 in central\n",
      "\tfound com.github.universal-automata#liblevenshtein;3.0.0 in central\n",
      "\tfound com.google.protobuf#protobuf-java-util;3.0.0-beta-3 in central\n",
      "\tfound com.google.protobuf#protobuf-java;3.0.0-beta-3 in central\n",
      "\tfound com.google.code.gson#gson;2.3 in central\n",
      "\tfound it.unimi.dsi#fastutil;7.0.12 in central\n",
      "\tfound org.projectlombok#lombok;1.16.8 in central\n",
      "\tfound com.google.cloud#google-cloud-storage;2.20.1 in central\n",
      "\tfound com.google.guava#guava;31.1-jre in central\n",
      "\tfound com.google.guava#failureaccess;1.0.1 in central\n",
      "\tfound com.google.guava#listenablefuture;9999.0-empty-to-avoid-conflict-with-guava in central\n",
      "\tfound com.google.errorprone#error_prone_annotations;2.18.0 in central\n",
      "\tfound com.google.j2objc#j2objc-annotations;1.3 in central\n",
      "\tfound com.google.http-client#google-http-client;1.43.0 in central\n",
      "\tfound io.opencensus#opencensus-contrib-http-util;0.31.1 in central\n",
      "\tfound com.google.http-client#google-http-client-jackson2;1.43.0 in central\n",
      "\tfound com.google.http-client#google-http-client-gson;1.43.0 in central\n",
      "\tfound com.google.api-client#google-api-client;2.2.0 in central\n",
      "\tfound com.google.oauth-client#google-oauth-client;1.34.1 in central\n",
      "\tfound com.google.http-client#google-http-client-apache-v2;1.43.0 in central\n",
      "\tfound com.google.apis#google-api-services-storage;v1-rev20220705-2.0.0 in central\n",
      "\tfound com.google.code.gson#gson;2.10.1 in central\n",
      "\tfound com.google.cloud#google-cloud-core;2.12.0 in central\n",
      "\tfound io.grpc#grpc-context;1.53.0 in central\n",
      "\tfound com.google.auto.value#auto-value-annotations;1.10.1 in central\n",
      "\tfound com.google.auto.value#auto-value;1.10.1 in central\n",
      "\tfound javax.annotation#javax.annotation-api;1.3.2 in central\n",
      "\tfound com.google.cloud#google-cloud-core-http;2.12.0 in central\n",
      "\tfound com.google.http-client#google-http-client-appengine;1.43.0 in central\n",
      "\tfound com.google.api#gax-httpjson;0.108.2 in central\n",
      "\tfound com.google.cloud#google-cloud-core-grpc;2.12.0 in central\n",
      "\tfound io.grpc#grpc-alts;1.53.0 in central\n",
      "\tfound io.grpc#grpc-grpclb;1.53.0 in central\n",
      "\tfound org.conscrypt#conscrypt-openjdk-uber;2.5.2 in central\n",
      "\tfound io.grpc#grpc-auth;1.53.0 in central\n",
      "\tfound io.grpc#grpc-protobuf;1.53.0 in central\n",
      "\tfound io.grpc#grpc-protobuf-lite;1.53.0 in central\n",
      "\tfound io.grpc#grpc-core;1.53.0 in central\n",
      "\tfound com.google.api#gax;2.23.2 in central\n",
      "\tfound com.google.api#gax-grpc;2.23.2 in central\n",
      "\tfound com.google.auth#google-auth-library-credentials;1.16.0 in central\n",
      "\tfound com.google.auth#google-auth-library-oauth2-http;1.16.0 in central\n",
      "\tfound com.google.api#api-common;2.6.2 in central\n",
      "\tfound io.opencensus#opencensus-api;0.31.1 in central\n",
      "\tfound com.google.api.grpc#proto-google-iam-v1;1.9.2 in central\n",
      "\tfound com.google.protobuf#protobuf-java;3.21.12 in central\n",
      "\tfound com.google.protobuf#protobuf-java-util;3.21.12 in central\n",
      "\tfound com.google.api.grpc#proto-google-common-protos;2.14.2 in central\n",
      "\tfound org.threeten#threetenbp;1.6.5 in central\n",
      "\tfound com.google.api.grpc#proto-google-cloud-storage-v2;2.20.1-alpha in central\n",
      "\tfound com.google.api.grpc#grpc-google-cloud-storage-v2;2.20.1-alpha in central\n",
      "\tfound com.google.api.grpc#gapic-google-cloud-storage-v2;2.20.1-alpha in central\n",
      "\tfound com.google.code.findbugs#jsr305;3.0.2 in central\n",
      "\tfound io.grpc#grpc-api;1.53.0 in central\n",
      "\tfound io.grpc#grpc-stub;1.53.0 in central\n",
      "\tfound org.checkerframework#checker-qual;3.31.0 in central\n",
      "\tfound io.perfmark#perfmark-api;0.26.0 in central\n",
      "\tfound com.google.android#annotations;4.1.1.4 in central\n",
      "\tfound org.codehaus.mojo#animal-sniffer-annotations;1.22 in central\n",
      "\tfound io.opencensus#opencensus-proto;0.2.0 in central\n",
      "\tfound io.grpc#grpc-services;1.53.0 in central\n",
      "\tfound com.google.re2j#re2j;1.6 in central\n",
      "\tfound io.grpc#grpc-netty-shaded;1.53.0 in central\n",
      "\tfound io.grpc#grpc-googleapis;1.53.0 in central\n",
      "\tfound io.grpc#grpc-xds;1.53.0 in central\n",
      "\tfound com.navigamez#greex;1.0 in central\n",
      "\tfound dk.brics.automaton#automaton;1.11-8 in central\n",
      "\tfound org.jsoup#jsoup;1.18.2 in central\n",
      "\tfound jakarta.mail#jakarta.mail-api;2.1.3 in central\n",
      "\tfound jakarta.activation#jakarta.activation-api;2.1.3 in central\n",
      "\tfound org.eclipse.angus#angus-mail;2.0.3 in central\n",
      "\tfound org.eclipse.angus#angus-activation;2.0.2 in central\n",
      "\tfound org.apache.poi#poi-ooxml;4.1.2 in central\n",
      "\tfound org.apache.poi#poi;4.1.2 in central\n",
      "\tfound org.apache.commons#commons-collections4;4.4 in central\n",
      "\tfound org.apache.commons#commons-math3;3.6.1 in central\n",
      "\tfound com.zaxxer#SparseBitSet;1.2 in central\n",
      "\tfound org.apache.poi#poi-ooxml-schemas;4.1.2 in central\n",
      "\tfound org.apache.xmlbeans#xmlbeans;3.1.0 in central\n",
      "\tfound org.apache.commons#commons-compress;1.19 in central\n",
      "\tfound com.github.virtuald#curvesapi;1.06 in central\n",
      "\tfound org.apache.poi#poi-scratchpad;4.1.2 in central\n",
      "\tfound com.johnsnowlabs.nlp#tensorflow-cpu_2.12;0.4.4 in central\n",
      "\tfound com.microsoft.onnxruntime#onnxruntime;1.19.2 in central\n",
      "\tfound com.johnsnowlabs.nlp#jsl-llamacpp-cpu_2.12;0.1.4 in central\n",
      "\tfound org.jetbrains#annotations;24.1.0 in central\n",
      "\tfound com.johnsnowlabs.nlp#jsl-openvino-cpu_2.12;0.1.0 in central\n",
      ":: resolution report :: resolve 691ms :: artifacts dl 14ms\n",
      "\t:: modules in use:\n",
      "\tcom.amazonaws#aws-java-sdk-core;1.12.500 from central in [default]\n",
      "\tcom.amazonaws#aws-java-sdk-kms;1.12.500 from central in [default]\n",
      "\tcom.amazonaws#aws-java-sdk-s3;1.12.500 from central in [default]\n",
      "\tcom.amazonaws#jmespath-java;1.12.500 from central in [default]\n",
      "\tcom.github.universal-automata#liblevenshtein;3.0.0 from central in [default]\n",
      "\tcom.github.virtuald#curvesapi;1.06 from central in [default]\n",
      "\tcom.google.android#annotations;4.1.1.4 from central in [default]\n",
      "\tcom.google.api#api-common;2.6.2 from central in [default]\n",
      "\tcom.google.api#gax;2.23.2 from central in [default]\n",
      "\tcom.google.api#gax-grpc;2.23.2 from central in [default]\n",
      "\tcom.google.api#gax-httpjson;0.108.2 from central in [default]\n",
      "\tcom.google.api-client#google-api-client;2.2.0 from central in [default]\n",
      "\tcom.google.api.grpc#gapic-google-cloud-storage-v2;2.20.1-alpha from central in [default]\n",
      "\tcom.google.api.grpc#grpc-google-cloud-storage-v2;2.20.1-alpha from central in [default]\n",
      "\tcom.google.api.grpc#proto-google-cloud-storage-v2;2.20.1-alpha from central in [default]\n",
      "\tcom.google.api.grpc#proto-google-common-protos;2.14.2 from central in [default]\n",
      "\tcom.google.api.grpc#proto-google-iam-v1;1.9.2 from central in [default]\n",
      "\tcom.google.apis#google-api-services-storage;v1-rev20220705-2.0.0 from central in [default]\n",
      "\tcom.google.auth#google-auth-library-credentials;1.16.0 from central in [default]\n",
      "\tcom.google.auth#google-auth-library-oauth2-http;1.16.0 from central in [default]\n",
      "\tcom.google.auto.value#auto-value;1.10.1 from central in [default]\n",
      "\tcom.google.auto.value#auto-value-annotations;1.10.1 from central in [default]\n",
      "\tcom.google.cloud#google-cloud-core;2.12.0 from central in [default]\n",
      "\tcom.google.cloud#google-cloud-core-grpc;2.12.0 from central in [default]\n",
      "\tcom.google.cloud#google-cloud-core-http;2.12.0 from central in [default]\n",
      "\tcom.google.cloud#google-cloud-storage;2.20.1 from central in [default]\n",
      "\tcom.google.code.findbugs#jsr305;3.0.2 from central in [default]\n",
      "\tcom.google.code.gson#gson;2.10.1 from central in [default]\n",
      "\tcom.google.errorprone#error_prone_annotations;2.18.0 from central in [default]\n",
      "\tcom.google.guava#failureaccess;1.0.1 from central in [default]\n",
      "\tcom.google.guava#guava;31.1-jre from central in [default]\n",
      "\tcom.google.guava#listenablefuture;9999.0-empty-to-avoid-conflict-with-guava from central in [default]\n",
      "\tcom.google.http-client#google-http-client;1.43.0 from central in [default]\n",
      "\tcom.google.http-client#google-http-client-apache-v2;1.43.0 from central in [default]\n",
      "\tcom.google.http-client#google-http-client-appengine;1.43.0 from central in [default]\n",
      "\tcom.google.http-client#google-http-client-gson;1.43.0 from central in [default]\n",
      "\tcom.google.http-client#google-http-client-jackson2;1.43.0 from central in [default]\n",
      "\tcom.google.j2objc#j2objc-annotations;1.3 from central in [default]\n",
      "\tcom.google.oauth-client#google-oauth-client;1.34.1 from central in [default]\n",
      "\tcom.google.protobuf#protobuf-java;3.21.12 from central in [default]\n",
      "\tcom.google.protobuf#protobuf-java-util;3.21.12 from central in [default]\n",
      "\tcom.google.re2j#re2j;1.6 from central in [default]\n",
      "\tcom.johnsnowlabs.nlp#jsl-llamacpp-cpu_2.12;0.1.4 from central in [default]\n",
      "\tcom.johnsnowlabs.nlp#jsl-openvino-cpu_2.12;0.1.0 from central in [default]\n",
      "\tcom.johnsnowlabs.nlp#spark-nlp_2.12;5.5.3 from central in [default]\n",
      "\tcom.johnsnowlabs.nlp#tensorflow-cpu_2.12;0.4.4 from central in [default]\n",
      "\tcom.microsoft.onnxruntime#onnxruntime;1.19.2 from central in [default]\n",
      "\tcom.navigamez#greex;1.0 from central in [default]\n",
      "\tcom.typesafe#config;1.4.2 from central in [default]\n",
      "\tcom.zaxxer#SparseBitSet;1.2 from central in [default]\n",
      "\tcommons-codec#commons-codec;1.15 from central in [default]\n",
      "\tcommons-logging#commons-logging;1.1.3 from central in [default]\n",
      "\tdk.brics.automaton#automaton;1.11-8 from central in [default]\n",
      "\tio.grpc#grpc-alts;1.53.0 from central in [default]\n",
      "\tio.grpc#grpc-api;1.53.0 from central in [default]\n",
      "\tio.grpc#grpc-auth;1.53.0 from central in [default]\n",
      "\tio.grpc#grpc-context;1.53.0 from central in [default]\n",
      "\tio.grpc#grpc-core;1.53.0 from central in [default]\n",
      "\tio.grpc#grpc-googleapis;1.53.0 from central in [default]\n",
      "\tio.grpc#grpc-grpclb;1.53.0 from central in [default]\n",
      "\tio.grpc#grpc-netty-shaded;1.53.0 from central in [default]\n",
      "\tio.grpc#grpc-protobuf;1.53.0 from central in [default]\n",
      "\tio.grpc#grpc-protobuf-lite;1.53.0 from central in [default]\n",
      "\tio.grpc#grpc-services;1.53.0 from central in [default]\n",
      "\tio.grpc#grpc-stub;1.53.0 from central in [default]\n",
      "\tio.grpc#grpc-xds;1.53.0 from central in [default]\n",
      "\tio.opencensus#opencensus-api;0.31.1 from central in [default]\n",
      "\tio.opencensus#opencensus-contrib-http-util;0.31.1 from central in [default]\n",
      "\tio.opencensus#opencensus-proto;0.2.0 from central in [default]\n",
      "\tio.perfmark#perfmark-api;0.26.0 from central in [default]\n",
      "\tit.unimi.dsi#fastutil;7.0.12 from central in [default]\n",
      "\tjakarta.activation#jakarta.activation-api;2.1.3 from central in [default]\n",
      "\tjakarta.mail#jakarta.mail-api;2.1.3 from central in [default]\n",
      "\tjavax.annotation#javax.annotation-api;1.3.2 from central in [default]\n",
      "\tjoda-time#joda-time;2.8.1 from central in [default]\n",
      "\torg.apache.commons#commons-collections4;4.4 from central in [default]\n",
      "\torg.apache.commons#commons-compress;1.19 from central in [default]\n",
      "\torg.apache.commons#commons-math3;3.6.1 from central in [default]\n",
      "\torg.apache.httpcomponents#httpclient;4.5.13 from central in [default]\n",
      "\torg.apache.httpcomponents#httpcore;4.4.13 from central in [default]\n",
      "\torg.apache.poi#poi;4.1.2 from central in [default]\n",
      "\torg.apache.poi#poi-ooxml;4.1.2 from central in [default]\n",
      "\torg.apache.poi#poi-ooxml-schemas;4.1.2 from central in [default]\n",
      "\torg.apache.poi#poi-scratchpad;4.1.2 from central in [default]\n",
      "\torg.apache.xmlbeans#xmlbeans;3.1.0 from central in [default]\n",
      "\torg.checkerframework#checker-qual;3.31.0 from central in [default]\n",
      "\torg.codehaus.mojo#animal-sniffer-annotations;1.22 from central in [default]\n",
      "\torg.conscrypt#conscrypt-openjdk-uber;2.5.2 from central in [default]\n",
      "\torg.eclipse.angus#angus-activation;2.0.2 from central in [default]\n",
      "\torg.eclipse.angus#angus-mail;2.0.3 from central in [default]\n",
      "\torg.jetbrains#annotations;24.1.0 from central in [default]\n",
      "\torg.jsoup#jsoup;1.18.2 from central in [default]\n",
      "\torg.projectlombok#lombok;1.16.8 from central in [default]\n",
      "\torg.rocksdb#rocksdbjni;6.29.5 from central in [default]\n",
      "\torg.threeten#threetenbp;1.6.5 from central in [default]\n",
      "\tsoftware.amazon.ion#ion-java;1.0.2 from central in [default]\n",
      "\t:: evicted modules:\n",
      "\tcommons-logging#commons-logging;1.2 by [commons-logging#commons-logging;1.1.3] in [default]\n",
      "\tcommons-codec#commons-codec;1.11 by [commons-codec#commons-codec;1.15] in [default]\n",
      "\tcom.google.protobuf#protobuf-java-util;3.0.0-beta-3 by [com.google.protobuf#protobuf-java-util;3.21.12] in [default]\n",
      "\tcom.google.protobuf#protobuf-java;3.0.0-beta-3 by [com.google.protobuf#protobuf-java;3.21.12] in [default]\n",
      "\tcom.google.code.gson#gson;2.3 by [com.google.code.gson#gson;2.10.1] in [default]\n",
      "\tcommons-codec#commons-codec;1.13 by [commons-codec#commons-codec;1.15] in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |  102  |   0   |   0   |   6   ||   96  |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-f8939018-dba4-43b1-9516-96ecd616a606\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 96 already retrieved (0kB/6ms)\n",
      "25/03/04 02:01:02 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/03/04 02:01:02 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "# 1. Initialize Spark Session\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName(\"EDGARCorpus\")\n",
    "    .master(\"local[*]\")\n",
    "    .config(\"spark.driver.memory\", \"8g\")\n",
    "    .config(\"spark.executor.memory\", \"32g\")\n",
    "    .config(\"spark.jars.packages\", \"com.johnsnowlabs.nlp:spark-nlp_2.12:5.5.3\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"OFF\")\n",
    "spark.conf.set(\"spark.sql.repl.eagerEval.enabled\", True)\n",
    "spark.sparkContext.addPyFile(\"utils/section_processing.py\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ea0e7ad2-2328-49cb-b179-9d9c3faa9305",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_sampled_cik_data(\n",
    "    years: List[int],\n",
    "    spark: SparkSession,\n",
    "    batch_size: int = 300,\n",
    "    output_dir: str = \"parquet_batches_task2\",\n",
    "    dataset_name: str = \"eloukas/edgar-corpus\",\n",
    "    split: str = \"train\",\n",
    "    random_seed: int = 66\n",
    "):\n",
    "    \"\"\"\n",
    "    Load multiple 'year_*' configurations for a given Hugging Face dataset, \n",
    "    convert them to Spark, ensure only 'cik' with all specified years are retained,\n",
    "    then randomly pick a single 'cik' and return a subset Spark DataFrame.\n",
    "    \n",
    "    Args:\n",
    "        years (List[int]): List of year values, e.g. [2018, 2019, 2020].\n",
    "        spark (SparkSession): An active Spark session.\n",
    "        batch_size (int, optional): Number of HF rows to process per batch \n",
    "                                    when converting to Spark. Defaults to 300.\n",
    "        output_dir (str, optional): Path where parquet batches are written \n",
    "                                    during conversion. Defaults to \"parquet_batches_task2\".\n",
    "        dataset_name (str, optional): Dataset name on Hugging Face hub. \n",
    "                                      Defaults to \"eloukas/edgar-corpus\".\n",
    "        split (str, optional): The split name to load (e.g., \"train\"). Defaults to \"train\".\n",
    "        random_seed (int, optional): Seed for reproducible random sampling. Defaults to 66.\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame: A Spark DataFrame containing only one sampled 'cik' \n",
    "                   that has data for all requested years.\n",
    "    \n",
    "    Raises:\n",
    "        ValueError: If no data could be loaded for the given years, \n",
    "                    or no 'cik' has all specified years.\n",
    "    \"\"\"\n",
    "    # -------------------------------------------------------\n",
    "    # Step 1: Load multiple Hugging Face 'year_*' configs and concatenate\n",
    "    # -------------------------------------------------------\n",
    "    dataset_list = []\n",
    "    for year in years:\n",
    "        config_name = f\"year_{year}\"\n",
    "        try:\n",
    "            ds = load_dataset(dataset_name, config_name, split=split)\n",
    "            dataset_list.append(ds)\n",
    "            print(f\"Loaded data for year={year}.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Could not load data for year={year}. Error: {e}\")\n",
    "\n",
    "    if not dataset_list:\n",
    "        raise ValueError(f\"No datasets were loaded successfully for years={years}.\")\n",
    "\n",
    "    combined_dataset = concatenate_datasets(dataset_list)\n",
    "    print(f\"Successfully concatenated {len(dataset_list)} dataset(s).\")\n",
    "\n",
    "    # -------------------------------------------------------\n",
    "    # Step 2: Convert the combined HF dataset to Spark\n",
    "    # -------------------------------------------------------\n",
    "    spark_df = convert_dataset_to_spark_df(\n",
    "        combined_dataset,\n",
    "        spark,\n",
    "        batch_size=batch_size,\n",
    "        output_dir=output_dir\n",
    "    )\n",
    "    print(f\"Converted dataset to Spark DataFrame with {spark_df.count()} rows.\")\n",
    "\n",
    "    # -------------------------------------------------------\n",
    "    # Step 3: Filter 'cik' with complete coverage of all years\n",
    "    # -------------------------------------------------------\n",
    "    grouped_df = spark_df.groupBy(\"cik\").agg(F.collect_set(\"year\").alias(\"years\"))\n",
    "    valid_ciks = grouped_df.filter(F.size(\"years\") == len(years))\n",
    "\n",
    "    # If none has all the specified years, raise an error\n",
    "    if valid_ciks.count() == 0:\n",
    "        raise ValueError(\n",
    "            f\"No 'cik' found that contains data for all these years: {years}.\"\n",
    "        )\n",
    "\n",
    "    # -------------------------------------------------------\n",
    "    # Step 4: Randomly pick one 'cik'\n",
    "    # -------------------------------------------------------\n",
    "    sampled_cik_row = valid_ciks.orderBy(F.rand(random_seed)).limit(1).first()\n",
    "    sampled_cik = sampled_cik_row[\"cik\"]\n",
    "    print(f\"Sampled CIK is: {sampled_cik}\")\n",
    "\n",
    "    # -------------------------------------------------------\n",
    "    # Step 5: Filter the original Spark DataFrame to just that 'cik' + the chosen years\n",
    "    # -------------------------------------------------------\n",
    "    sampled_df = spark_df.filter(\n",
    "        (F.col(\"cik\") == sampled_cik)\n",
    "        & (F.col(\"year\").isin(years))\n",
    "    )\n",
    "\n",
    "    sampled_df.cache()\n",
    "    final_count = sampled_df.count()\n",
    "    print(f\"Final sampled_df has {final_count} rows for CIK={sampled_cik} and years={years}.\")\n",
    "\n",
    "    # Return the final Spark DataFrame for further processing\n",
    "    return sampled_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7c118b4e-465a-45a4-ad3a-0e552b4f9a18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded data for year=2018.\n",
      "Loaded data for year=2019.\n",
      "Loaded data for year=2020.\n",
      "Successfully concatenated 3 dataset(s).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aea2d68013124a15ae5b11d10ddccf18",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Writing batches to Parquet:   0%|          | 0/16342 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted dataset to Spark DataFrame with 16342 rows.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampled CIK is: 1035976\n",
      "Final sampled_df has 3 rows for CIK=1035976 and years=[2018, 2019, 2020].\n"
     ]
    }
   ],
   "source": [
    "years_list = [2018, 2019, 2020]\n",
    "sampled_df = load_sampled_cik_data(\n",
    "    years=years_list,\n",
    "    spark=spark,\n",
    "    batch_size=300,\n",
    "    output_dir=\"parquet_batches_task2\",\n",
    "    dataset_name=\"eloukas/edgar-corpus\",\n",
    "    split=\"train\",\n",
    "    random_seed=66\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f59f9982-1a1f-4bff-84b2-2df25ebc79b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- filename: string (nullable = true)\n",
      " |-- cik: string (nullable = true)\n",
      " |-- year: string (nullable = true)\n",
      " |-- section_1: string (nullable = true)\n",
      " |-- section_1A: string (nullable = true)\n",
      " |-- section_1B: string (nullable = true)\n",
      " |-- section_2: string (nullable = true)\n",
      " |-- section_3: string (nullable = true)\n",
      " |-- section_4: string (nullable = true)\n",
      " |-- section_5: string (nullable = true)\n",
      " |-- section_6: string (nullable = true)\n",
      " |-- section_7: string (nullable = true)\n",
      " |-- section_7A: string (nullable = true)\n",
      " |-- section_8: string (nullable = true)\n",
      " |-- section_9: string (nullable = true)\n",
      " |-- section_9A: string (nullable = true)\n",
      " |-- section_9B: string (nullable = true)\n",
      " |-- section_10: string (nullable = true)\n",
      " |-- section_11: string (nullable = true)\n",
      " |-- section_12: string (nullable = true)\n",
      " |-- section_13: string (nullable = true)\n",
      " |-- section_14: string (nullable = true)\n",
      " |-- section_15: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sampled_df.printSchema()\n",
    "sampled_df.cache()\n",
    "_ = sampled_df.count()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9d7bb252-1a96-4fd0-856d-ef3790843578",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>filename</th><th>cik</th><th>year</th><th>section_1</th><th>section_1A</th><th>section_1B</th><th>section_2</th><th>section_3</th><th>section_4</th><th>section_5</th><th>section_6</th><th>section_7</th><th>section_7A</th><th>section_8</th><th>section_9</th><th>section_9A</th><th>section_9B</th><th>section_10</th><th>section_11</th><th>section_12</th><th>section_13</th><th>section_14</th><th>section_15</th></tr>\n",
       "<tr><td>1035976_2020.htm</td><td>1035976</td><td>2020</td><td>Item 1.\\nBusiness...</td><td>Item 1A.\\nRisk Fa...</td><td>Item 1B.\\nUnresol...</td><td>Item 2.\\nProperti...</td><td>Item 3.\\nLegal Pr...</td><td>Item 4.\\nMine Saf...</td><td>Item 5.\\nMarket f...</td><td>Item 6.\\nSelected...</td><td>Item 7.\\nManageme...</td><td>Item 7A. Quantita...</td><td>Item 8. Financial...</td><td>Item 9. Changes i...</td><td>Item 9A. Controls...</td><td>Item 9B.\\nOther I...</td><td>Item 10.\\nDirecto...</td><td>Item 11.\\nExecuti...</td><td>Item 12.\\nSecurit...</td><td>Item 13.\\nCertain...</td><td>Item 14.\\nPrincip...</td><td>Item 15.\\nExhibit...</td></tr>\n",
       "<tr><td>1035976_2018.htm</td><td>1035976</td><td>2018</td><td>Item 1.\\nBusiness...</td><td>Item 1A.\\nRisk Fa...</td><td>Item 1B.\\nUnresol...</td><td>Item 2.\\nProperti...</td><td>Item 3.\\nLegal Pr...</td><td>Item 4.\\nMine Saf...</td><td>Item 5.\\nMarket f...</td><td>Item 6.\\nSelected...</td><td>Item 7.\\nManageme...</td><td>Item 7A. Quantita...</td><td>Item 8. Financial...</td><td>Item 9. Changes i...</td><td>Item 9A. Controls...</td><td>Item 9B.\\nOther I...</td><td>Item 10.\\nDirecto...</td><td>Item 11.\\nExecuti...</td><td>Item 12.\\nSecurit...</td><td>Item 13.\\nCertain...</td><td>Item 14.\\nPrincip...</td><td>Item 15.\\nExhibit...</td></tr>\n",
       "<tr><td>1035976_2019.htm</td><td>1035976</td><td>2019</td><td>Item 1.\\nBusiness...</td><td>Item 1A.\\nRisk Fa...</td><td>Item 1B.\\nUnresol...</td><td>Item 2.\\nProperti...</td><td>Item 3.\\nLegal Pr...</td><td>Item 4.\\nMine Saf...</td><td>Item 5.\\nMarket f...</td><td>Item 6.\\nSelected...</td><td>Item 7.\\nManageme...</td><td>Item 7A. Quantita...</td><td>Item 8. Financial...</td><td>Item 9. Changes i...</td><td>Item 9A. Controls...</td><td>Item 9B.\\nOther I...</td><td>Item 10.\\nDirecto...</td><td>Item 11.\\nExecuti...</td><td>Item 12.\\nSecurit...</td><td>Item 13.\\nCertain...</td><td>Item 14.\\nPrincip...</td><td>Item 15.\\nExhibit...</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+----------------+-------+----+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
       "|        filename|    cik|year|           section_1|          section_1A|          section_1B|           section_2|           section_3|           section_4|           section_5|           section_6|           section_7|          section_7A|           section_8|           section_9|          section_9A|          section_9B|          section_10|          section_11|          section_12|          section_13|          section_14|          section_15|\n",
       "+----------------+-------+----+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
       "|1035976_2020.htm|1035976|2020|Item 1.\\nBusiness...|Item 1A.\\nRisk Fa...|Item 1B.\\nUnresol...|Item 2.\\nProperti...|Item 3.\\nLegal Pr...|Item 4.\\nMine Saf...|Item 5.\\nMarket f...|Item 6.\\nSelected...|Item 7.\\nManageme...|Item 7A. Quantita...|Item 8. Financial...|Item 9. Changes i...|Item 9A. Controls...|Item 9B.\\nOther I...|Item 10.\\nDirecto...|Item 11.\\nExecuti...|Item 12.\\nSecurit...|Item 13.\\nCertain...|Item 14.\\nPrincip...|Item 15.\\nExhibit...|\n",
       "|1035976_2018.htm|1035976|2018|Item 1.\\nBusiness...|Item 1A.\\nRisk Fa...|Item 1B.\\nUnresol...|Item 2.\\nProperti...|Item 3.\\nLegal Pr...|Item 4.\\nMine Saf...|Item 5.\\nMarket f...|Item 6.\\nSelected...|Item 7.\\nManageme...|Item 7A. Quantita...|Item 8. Financial...|Item 9. Changes i...|Item 9A. Controls...|Item 9B.\\nOther I...|Item 10.\\nDirecto...|Item 11.\\nExecuti...|Item 12.\\nSecurit...|Item 13.\\nCertain...|Item 14.\\nPrincip...|Item 15.\\nExhibit...|\n",
       "|1035976_2019.htm|1035976|2019|Item 1.\\nBusiness...|Item 1A.\\nRisk Fa...|Item 1B.\\nUnresol...|Item 2.\\nProperti...|Item 3.\\nLegal Pr...|Item 4.\\nMine Saf...|Item 5.\\nMarket f...|Item 6.\\nSelected...|Item 7.\\nManageme...|Item 7A. Quantita...|Item 8. Financial...|Item 9. Changes i...|Item 9A. Controls...|Item 9B.\\nOther I...|Item 10.\\nDirecto...|Item 11.\\nExecuti...|Item 12.\\nSecurit...|Item 13.\\nCertain...|Item 14.\\nPrincip...|Item 15.\\nExhibit...|\n",
       "+----------------+-------+----+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampled_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1601bad9-6f5f-4c8b-bdde-2f09ce150ff7",
   "metadata": {},
   "source": [
    "#  Convert Documents into Chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "54d6d9c5-1775-43bb-a3d6-1c9fede465ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dcd028fa135248c2aa72d45fae3b37a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing sections:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 282 chunk columns.\n"
     ]
    }
   ],
   "source": [
    "# 1. Convert Documents into Chunks\n",
    "processor = SectionProcessor(max_words=300, overlap=1)\n",
    "\n",
    "processed_df = processor.process_all_sections(\n",
    "    sampled_df, \n",
    "    show_progress=True,\n",
    "    force_action=False\n",
    ")\n",
    "\n",
    "chunk_cols = [c for c in processed_df.columns if \"_chunk_\" in c]\n",
    "print(f\"Found {len(chunk_cols)} chunk columns.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1ced5778-a973-4cd7-94bc-645538b3490d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- cik: string (nullable = true)\n",
      " |-- year: string (nullable = true)\n",
      " |-- chunk_col_name: string (nullable = true)\n",
      " |-- text: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# Stack chunk columns into long format: (chunk_col_name, text)\n",
    "stack_expr = \"stack({0}, {1}) as (chunk_col_name, text)\".format(\n",
    "    len(chunk_cols),\n",
    "    \", \".join([f\"'{col}', {col}\" for col in chunk_cols])\n",
    ")\n",
    "\n",
    "df_long = processed_df.select(\"cik\", \"year\", expr(stack_expr))\n",
    "df_long = df_long.filter(df_long[\"text\"].isNotNull())\n",
    "\n",
    "df_long.printSchema()\n",
    "df_long.cache()\n",
    "_ = df_long.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0a770975-a9c7-4111-8118-fb6b9ce2f52a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>cik</th><th>year</th><th>chunk_col_name</th><th>text</th></tr>\n",
       "<tr><td>1035976</td><td>2020</td><td>section_1_chunk_1</td><td>Item 1.\\nBusiness...</td></tr>\n",
       "<tr><td>1035976</td><td>2020</td><td>section_1_chunk_2</td><td>●\\nPeople - A tea...</td></tr>\n",
       "<tr><td>1035976</td><td>2020</td><td>section_1_chunk_3</td><td>Telephone banking...</td></tr>\n",
       "<tr><td>1035976</td><td>2020</td><td>section_1_chunk_4</td><td>The Bank also off...</td></tr>\n",
       "<tr><td>1035976</td><td>2020</td><td>section_1_chunk_5</td><td>Residential Mortg...</td></tr>\n",
       "<tr><td>1035976</td><td>2020</td><td>section_1_chunk_6</td><td>At December 31, 2...</td></tr>\n",
       "<tr><td>1035976</td><td>2020</td><td>section_1_chunk_7</td><td>FNCB also partici...</td></tr>\n",
       "<tr><td>1035976</td><td>2020</td><td>section_1_chunk_8</td><td>FNCB relies prima...</td></tr>\n",
       "<tr><td>1035976</td><td>2020</td><td>section_1_chunk_9</td><td>The cost of regul...</td></tr>\n",
       "<tr><td>1035976</td><td>2020</td><td>section_1_chunk_10</td><td>In general, these...</td></tr>\n",
       "<tr><td>1035976</td><td>2020</td><td>section_1_chunk_11</td><td>The BHCA requires...</td></tr>\n",
       "<tr><td>1035976</td><td>2020</td><td>section_1_chunk_12</td><td>Some of the activ...</td></tr>\n",
       "<tr><td>1035976</td><td>2020</td><td>section_1_chunk_13</td><td>FNCB has not elec...</td></tr>\n",
       "<tr><td>1035976</td><td>2020</td><td>section_1_chunk_14</td><td>The Dodd-Frank Ac...</td></tr>\n",
       "<tr><td>1035976</td><td>2020</td><td>section_1_chunk_15</td><td>Under the Uniting...</td></tr>\n",
       "<tr><td>1035976</td><td>2020</td><td>section_1_chunk_16</td><td>Basel III provide...</td></tr>\n",
       "<tr><td>1035976</td><td>2020</td><td>section_1_chunk_17</td><td>FNCB and the Bank...</td></tr>\n",
       "<tr><td>1035976</td><td>2020</td><td>section_1_chunk_18</td><td>The federal banki...</td></tr>\n",
       "<tr><td>1035976</td><td>2020</td><td>section_1_chunk_19</td><td>The Bank and its ...</td></tr>\n",
       "<tr><td>1035976</td><td>2020</td><td>section_1_chunk_20</td><td>The Dodd-Frank Ac...</td></tr>\n",
       "</table>\n",
       "only showing top 20 rows\n"
      ],
      "text/plain": [
       "+-------+----+------------------+--------------------+\n",
       "|    cik|year|    chunk_col_name|                text|\n",
       "+-------+----+------------------+--------------------+\n",
       "|1035976|2020| section_1_chunk_1|Item 1.\\nBusiness...|\n",
       "|1035976|2020| section_1_chunk_2|●\\nPeople - A tea...|\n",
       "|1035976|2020| section_1_chunk_3|Telephone banking...|\n",
       "|1035976|2020| section_1_chunk_4|The Bank also off...|\n",
       "|1035976|2020| section_1_chunk_5|Residential Mortg...|\n",
       "|1035976|2020| section_1_chunk_6|At December 31, 2...|\n",
       "|1035976|2020| section_1_chunk_7|FNCB also partici...|\n",
       "|1035976|2020| section_1_chunk_8|FNCB relies prima...|\n",
       "|1035976|2020| section_1_chunk_9|The cost of regul...|\n",
       "|1035976|2020|section_1_chunk_10|In general, these...|\n",
       "|1035976|2020|section_1_chunk_11|The BHCA requires...|\n",
       "|1035976|2020|section_1_chunk_12|Some of the activ...|\n",
       "|1035976|2020|section_1_chunk_13|FNCB has not elec...|\n",
       "|1035976|2020|section_1_chunk_14|The Dodd-Frank Ac...|\n",
       "|1035976|2020|section_1_chunk_15|Under the Uniting...|\n",
       "|1035976|2020|section_1_chunk_16|Basel III provide...|\n",
       "|1035976|2020|section_1_chunk_17|FNCB and the Bank...|\n",
       "|1035976|2020|section_1_chunk_18|The federal banki...|\n",
       "|1035976|2020|section_1_chunk_19|The Bank and its ...|\n",
       "|1035976|2020|section_1_chunk_20|The Dodd-Frank Ac...|\n",
       "+-------+----+------------------+--------------------+\n",
       "only showing top 20 rows"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_long"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3249bbb-4e0f-464f-af2f-8ede59151ef6",
   "metadata": {},
   "source": [
    "# Convert Chunks to Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "354fcd53-e6a4-4bab-93c8-ab932196f1cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e5_large download started this may take some time.\n",
      "Approximate size to download 759.2 MB\n",
      "e5_large download started this may take some time.\n",
      "Approximate size to download 759.2 MB\n",
      "Download done! Loading the resource.\n",
      "[ / ]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.util.SizeEstimator$ (file:/Users/yuxiangwang/venv/lib/python3.13/site-packages/pyspark/jars/spark-core_2.12-3.5.5.jar) to field java.nio.charset.Charset.name\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.util.SizeEstimator$\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK!]\n"
     ]
    }
   ],
   "source": [
    "# Create a single pipeline with DocumentAssembler + E5 embeddings\n",
    "document_assembler = DocumentAssembler().setInputCol(\"text\").setOutputCol(\"document\")\n",
    "embedding = E5Embeddings.pretrained(\"e5_large\", \"en\") \\\n",
    "    .setInputCols([\"document\"]) \\\n",
    "    .setOutputCol(\"embeddings\")\n",
    "\n",
    "pipeline = Pipeline().setStages([document_assembler, embedding])\n",
    "\n",
    "# because E5Embeddings does not actually learn from data in the pipeline.\n",
    "fit_sample_df = df_long.limit(1)\n",
    "pipeline_model = pipeline.fit(fit_sample_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "46b707e2-86ba-45c7-aa70-58084fa06596",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_embeddings(input_df, pipeline_model):\n",
    "    \"\"\"\n",
    "    Transform the input_df using a pre-fitted pipeline_model.\n",
    "    Extract the embedding array into a Spark ML Vector column 'embeddings_vector'.\n",
    "    \"\"\"\n",
    "    transformed_df = pipeline_model.transform(input_df)\n",
    "    \n",
    "    # UDF to convert python array -> Spark ML DenseVector\n",
    "    def array_to_vector(arr):\n",
    "        return Vectors.dense(arr) if arr else Vectors.dense([])\n",
    "    to_vector_udf = udf(array_to_vector, VectorUDT())\n",
    "    \n",
    "    # Extract embeddings from the annotation\n",
    "    df_embeddings = (\n",
    "        transformed_df\n",
    "        .withColumn(\"embeddings_array\", expr(\"transform(embeddings, x -> x.embeddings)[0]\"))\n",
    "        .withColumn(\"embeddings_vector\", to_vector_udf(\"embeddings_array\"))\n",
    "        .select(\"cik\", \"year\", \"chunk_col_name\", \"text\", \"embeddings_vector\")\n",
    "    )\n",
    "    \n",
    "    return df_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a4a0a2a0-63cd-49c2-b220-06ad53e540f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_embeddings = transform_embeddings(df_long, pipeline_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c809e15d-de6a-4e6e-9203-834302b221df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- cik: string (nullable = true)\n",
      " |-- year: string (nullable = true)\n",
      " |-- chunk_col_name: string (nullable = true)\n",
      " |-- text: string (nullable = true)\n",
      " |-- embeddings_vector: vector (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 91:===========================================>            (14 + 4) / 18]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CPUs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "df_embeddings.printSchema()\n",
    "\n",
    "df_embeddings.cache()\n",
    "_ = df_embeddings.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fa28a830-cd6d-44f1-9036-24752d9219da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>cik</th><th>year</th><th>chunk_col_name</th><th>text</th><th>embeddings_vector</th></tr>\n",
       "<tr><td>1035976</td><td>2020</td><td>section_1_chunk_1</td><td>Item 1.\\nBusiness...</td><td>[0.00406089192256...</td></tr>\n",
       "<tr><td>1035976</td><td>2020</td><td>section_1_chunk_2</td><td>●\\nPeople - A tea...</td><td>[0.00559781398624...</td></tr>\n",
       "<tr><td>1035976</td><td>2020</td><td>section_1_chunk_3</td><td>Telephone banking...</td><td>[-0.0019283706787...</td></tr>\n",
       "<tr><td>1035976</td><td>2020</td><td>section_1_chunk_4</td><td>The Bank also off...</td><td>[0.00454328162595...</td></tr>\n",
       "<tr><td>1035976</td><td>2020</td><td>section_1_chunk_5</td><td>Residential Mortg...</td><td>[0.00315549760125...</td></tr>\n",
       "<tr><td>1035976</td><td>2020</td><td>section_1_chunk_6</td><td>At December 31, 2...</td><td>[-0.0166966784745...</td></tr>\n",
       "<tr><td>1035976</td><td>2020</td><td>section_1_chunk_7</td><td>FNCB also partici...</td><td>[0.00743138557299...</td></tr>\n",
       "<tr><td>1035976</td><td>2020</td><td>section_1_chunk_8</td><td>FNCB relies prima...</td><td>[0.00184497167356...</td></tr>\n",
       "<tr><td>1035976</td><td>2020</td><td>section_1_chunk_9</td><td>The cost of regul...</td><td>[-0.0049952873960...</td></tr>\n",
       "<tr><td>1035976</td><td>2020</td><td>section_1_chunk_10</td><td>In general, these...</td><td>[-0.0050230352208...</td></tr>\n",
       "<tr><td>1035976</td><td>2020</td><td>section_1_chunk_11</td><td>The BHCA requires...</td><td>[-0.0042554843239...</td></tr>\n",
       "<tr><td>1035976</td><td>2020</td><td>section_1_chunk_12</td><td>Some of the activ...</td><td>[-0.0115933893248...</td></tr>\n",
       "<tr><td>1035976</td><td>2020</td><td>section_1_chunk_13</td><td>FNCB has not elec...</td><td>[-0.0085528986528...</td></tr>\n",
       "<tr><td>1035976</td><td>2020</td><td>section_1_chunk_14</td><td>The Dodd-Frank Ac...</td><td>[-0.0090536409988...</td></tr>\n",
       "<tr><td>1035976</td><td>2020</td><td>section_1_chunk_15</td><td>Under the Uniting...</td><td>[-0.0173722635954...</td></tr>\n",
       "<tr><td>1035976</td><td>2020</td><td>section_1_chunk_16</td><td>Basel III provide...</td><td>[-0.0135165741667...</td></tr>\n",
       "<tr><td>1035976</td><td>2020</td><td>section_1_chunk_17</td><td>FNCB and the Bank...</td><td>[-0.0189133603125...</td></tr>\n",
       "<tr><td>1035976</td><td>2020</td><td>section_1_chunk_18</td><td>The federal banki...</td><td>[-0.0253556296229...</td></tr>\n",
       "<tr><td>1035976</td><td>2020</td><td>section_1_chunk_19</td><td>The Bank and its ...</td><td>[-0.0192488636821...</td></tr>\n",
       "<tr><td>1035976</td><td>2020</td><td>section_1_chunk_20</td><td>The Dodd-Frank Ac...</td><td>[-0.0197582188993...</td></tr>\n",
       "</table>\n",
       "only showing top 20 rows\n"
      ],
      "text/plain": [
       "+-------+----+------------------+--------------------+--------------------+\n",
       "|    cik|year|    chunk_col_name|                text|   embeddings_vector|\n",
       "+-------+----+------------------+--------------------+--------------------+\n",
       "|1035976|2020| section_1_chunk_1|Item 1.\\nBusiness...|[0.00406089192256...|\n",
       "|1035976|2020| section_1_chunk_2|●\\nPeople - A tea...|[0.00559781398624...|\n",
       "|1035976|2020| section_1_chunk_3|Telephone banking...|[-0.0019283706787...|\n",
       "|1035976|2020| section_1_chunk_4|The Bank also off...|[0.00454328162595...|\n",
       "|1035976|2020| section_1_chunk_5|Residential Mortg...|[0.00315549760125...|\n",
       "|1035976|2020| section_1_chunk_6|At December 31, 2...|[-0.0166966784745...|\n",
       "|1035976|2020| section_1_chunk_7|FNCB also partici...|[0.00743138557299...|\n",
       "|1035976|2020| section_1_chunk_8|FNCB relies prima...|[0.00184497167356...|\n",
       "|1035976|2020| section_1_chunk_9|The cost of regul...|[-0.0049952873960...|\n",
       "|1035976|2020|section_1_chunk_10|In general, these...|[-0.0050230352208...|\n",
       "|1035976|2020|section_1_chunk_11|The BHCA requires...|[-0.0042554843239...|\n",
       "|1035976|2020|section_1_chunk_12|Some of the activ...|[-0.0115933893248...|\n",
       "|1035976|2020|section_1_chunk_13|FNCB has not elec...|[-0.0085528986528...|\n",
       "|1035976|2020|section_1_chunk_14|The Dodd-Frank Ac...|[-0.0090536409988...|\n",
       "|1035976|2020|section_1_chunk_15|Under the Uniting...|[-0.0173722635954...|\n",
       "|1035976|2020|section_1_chunk_16|Basel III provide...|[-0.0135165741667...|\n",
       "|1035976|2020|section_1_chunk_17|FNCB and the Bank...|[-0.0189133603125...|\n",
       "|1035976|2020|section_1_chunk_18|The federal banki...|[-0.0253556296229...|\n",
       "|1035976|2020|section_1_chunk_19|The Bank and its ...|[-0.0192488636821...|\n",
       "|1035976|2020|section_1_chunk_20|The Dodd-Frank Ac...|[-0.0197582188993...|\n",
       "+-------+----+------------------+--------------------+--------------------+\n",
       "only showing top 20 rows"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b70f57a-1b43-414e-a664-279a26228787",
   "metadata": {},
   "source": [
    "# Creating a query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d441f904-e586-44b6-bf2f-0b697c0a4db6",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_text = \"What is the company's net income for the year 2018?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2f128174-33cb-4b6b-8751-ec32620c474a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_query_embedding(query_text: str, pipeline_model) -> Vectors:\n",
    "    \"\"\"\n",
    "    Given a single query string, transform it with a pre-fitted Spark NLP\n",
    "    embedding pipeline model to obtain a Spark ML Vector.\n",
    "    \n",
    "    Args:\n",
    "        query_text (str): The input query text.\n",
    "        pipeline_model: A fitted Spark ML PipelineModel (including a text embedding stage).\n",
    "    \n",
    "    Returns:\n",
    "        Vectors: A Spark DenseVector containing the query embeddings.\n",
    "    \"\"\"\n",
    "    # Create a single-row DataFrame to hold the query text\n",
    "    df_query = spark.createDataFrame([(query_text, )], [\"text\"])\n",
    "    \n",
    "    # Transform the query text with the fitted pipeline model\n",
    "    transformed_query = pipeline_model.transform(df_query)\n",
    "    \n",
    "    # UDF to convert a Python array to Spark DenseVector\n",
    "    def array_to_vector(arr):\n",
    "        return Vectors.dense(arr) if arr else Vectors.dense([])\n",
    "    to_vector_udf = udf(array_to_vector, VectorUDT())\n",
    "    \n",
    "    # Extract the embeddings array from the annotation output\n",
    "    df_q_emb = (\n",
    "        transformed_query\n",
    "        .withColumn(\"embeddings_array\", expr(\"transform(embeddings, x -> x.embeddings)[0]\"))\n",
    "        .withColumn(\"query_vector\", to_vector_udf(col(\"embeddings_array\")))\n",
    "        .select(\"query_vector\")\n",
    "    )\n",
    "    \n",
    "    # Collect the single row\n",
    "    row = df_q_emb.collect()[0]\n",
    "    return row[\"query_vector\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fccfacc9-1574-45fd-b6ee-578b29afceee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_cosine_similarity_udf(query_vector: Vectors):\n",
    "    \"\"\"\n",
    "    Create a Spark UDF that calculates the cosine similarity between\n",
    "    an 'embeddings_vector' column and the provided query_vector.\n",
    "    \n",
    "    Args:\n",
    "        query_vector (Vectors): A Spark DenseVector representing the query embedding.\n",
    "    \n",
    "    Returns:\n",
    "        A pyspark SQL UDF that takes a DenseVector and returns a float (similarity).\n",
    "    \"\"\"\n",
    "    # Broadcast the query vector to worker nodes for efficient dot-product computation\n",
    "    bc_query_vec = spark.sparkContext.broadcast(query_vector)\n",
    "    \n",
    "    def cos_sim(chunk_vec):\n",
    "        \"\"\"\n",
    "        Compute the cosine similarity between chunk_vec and the broadcast query vector.\n",
    "        \"\"\"\n",
    "        dot_val = chunk_vec.dot(bc_query_vec.value)\n",
    "        norm_mult = chunk_vec.norm(2) * bc_query_vec.value.norm(2)\n",
    "        if norm_mult == 0.0:\n",
    "            return 0.0\n",
    "        return float(dot_val / norm_mult)\n",
    "    \n",
    "    return udf(cos_sim, DoubleType())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00ec948d-a3c3-4f6d-92eb-85891deb727e",
   "metadata": {},
   "source": [
    "# Create a prompt to extract data from chunks from a specific year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "81e7a1ed-d248-43c8-bc8b-19e1f4dd08d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Define a Pydantic model for the LLM output schema\n",
    "class QAPair(BaseModel):\n",
    "    \"\"\"\n",
    "    A simple schema with 'question' and 'answer' fields. after manual check can be used as val dataset.\n",
    "    \"\"\"\n",
    "    question: str\n",
    "    answer: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0c84479d-f1e9-4da1-813c-836557db58b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_qa_from_chunk(\n",
    "    year: str,\n",
    "    chunk_col_name: str,\n",
    "    chunk_text: str,\n",
    "    model: str = \"llama3.3\"\n",
    ") -> Optional[QAPair]:\n",
    "    \"\"\"\n",
    "    Calls Ollama with a structured JSON format prompt, asking the LLM\n",
    "    to return a JSON object matching QAPair's schema.\n",
    "    \n",
    "    Args:\n",
    "        year (str): The year for this text chunk, e.g. \"2018\".\n",
    "        chunk_col_name (str): The chunk column name or identifier.\n",
    "        chunk_text (str): The actual text of the chunk from the 10-K.\n",
    "        model (str): Ollama model name, default='llama3.3'.\n",
    "    \n",
    "    Returns:\n",
    "        QAPair: If parsing is successful, returns a QAPair object with\n",
    "                question and answer attributes.\n",
    "        None: If the response is not valid JSON or fails validation.\n",
    "    \"\"\"\n",
    "    \n",
    "    # 2. Build a user prompt that instructs the LLM to produce JSON\n",
    "    prompt = f\"\"\"You are a financial analysis assistant.\n",
    "You are given a 10-K chunk from a company's filing with following data:\n",
    "- Year: {year}\n",
    "- Chunk Name: {chunk_col_name}\n",
    "- Text Content:\n",
    "\n",
    "\\\"\\\"\\\"{chunk_text}\\\"\\\"\\\"\n",
    "\n",
    "From this chunk, please:\n",
    "#1. Formulate the MOST relevant or significant question about this chunk's financial data.\n",
    "#2. Provide an answer to that question, based on the chunk.\n",
    "\n",
    "Your response MUST be valid JSON in the format:\n",
    "{{\n",
    "  \"question\": \"...\",\n",
    "  \"answer\": \"...\"\n",
    "}}\n",
    "\n",
    "Do not include extra keys. No extra commentary—just valid JSON.\n",
    "\"\"\"\n",
    "\n",
    "    # 3. Call Ollama, requesting the QAPair schema as the output format\n",
    "    response = chat(\n",
    "        messages=[{'role': 'user', 'content': prompt}],\n",
    "        model=model,\n",
    "        format=QAPair.model_json_schema(),\n",
    "    )\n",
    "    #print(prompt)\n",
    "    # 4. Parse the JSON response using Pydantic\n",
    "    try:\n",
    "        qa_obj = QAPair.model_validate_json(response.message.content)\n",
    "        return qa_obj\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to parse JSON into QAPair: {e}\")\n",
    "        print(\"Raw LLM output was:\", response.message.content)\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f419315-e60d-45d7-8495-11c6a1dfc416",
   "metadata": {},
   "source": [
    "# Create a validation dataset (5 true values from chunks)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f78ea102-7dbc-469a-97b3-7ad4b0773692",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Randomly select 10 chunks from df_embeddings\n",
    "df_meta_val = df_embeddings.orderBy(rand(42)).limit(10)  \n",
    "rows = df_meta_val.collect()\n",
    "\n",
    "validation_data = []  # We'll store final results here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "394bc711-26e9-4fe5-b8e4-254fb9e01dc0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bbdf73fc6d214132941d8df463134a8a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting Q/A from chunks:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Step 2: For each chunk, call extract_qa_from_chunk with a progress bar\n",
    "for row in tqdm(rows, desc=\"Extracting Q/A from chunks\"):\n",
    "    # Convert 'year' to a string if necessary\n",
    "    year_str = str(row[\"year\"])\n",
    "    \n",
    "    chunk_name = row[\"chunk_col_name\"]\n",
    "    chunk_text = row[\"text\"]\n",
    "    \n",
    "    # Call the function that prompts the LLM to extract Q/A in JSON\n",
    "    qa_result = extract_qa_from_chunk(\n",
    "        year=year_str,\n",
    "        chunk_col_name=chunk_name,\n",
    "        chunk_text=chunk_text,\n",
    "        model=\"deepseek-r1:70b\"  # or another Ollama model if desired\n",
    "    )\n",
    "    \n",
    "    # If parsing was successful, store the result; otherwise store placeholders\n",
    "    if qa_result is not None:\n",
    "        validation_data.append({\n",
    "            \"year\": year_str,\n",
    "            \"chunk_col_name\": chunk_name,\n",
    "            \"text\": chunk_text,\n",
    "            \"question\": qa_result.question,\n",
    "            \"answer\": qa_result.answer\n",
    "        })\n",
    "    else:\n",
    "        validation_data.append({\n",
    "            \"year\": year_str,\n",
    "            \"chunk_col_name\": chunk_name,\n",
    "            \"text\": chunk_text,\n",
    "            \"question\": None,\n",
    "            \"answer\": None\n",
    "        })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6dfd119a-502e-431d-8acf-128c541bc21c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Validation Chunk #1 ===\n",
      "Year: 2020\n",
      "Chunk Column: section_8_chunk_75\n",
      "\n",
      "\n",
      "Question: What is the methodology used by FNCB to determine estimated fair value amounts, and what factors could affect these estimates?\n",
      "Answer: FNCB determines its estimated fair value amounts using available market information and appropriate valuation methodologies. However, management judgment is required to interpret data and develop these estimates. The use of different market assumptions and/or estimation methodologies may have a material effect on the estimated fair value amounts.\n",
      "\n",
      "\n",
      "---Text ---\n",
      "The following estimated fair value amounts have been determined using available market information and appropriate valuation methodologies. However, management judgment is required to interpret data and develop fair value estimates. Accordingly, the estimates below are not necessarily indicative of the amounts FNCB could realize in a current market exchange. The use of different market assumptions and/or estimation methodologies may have a material effect on the estimated fair value amounts. Note 17. EARNINGS PER SHARE\n",
      "For FNCB, the numerator of both the basic and diluted earnings per share of common stock is net income available to common shareholders. The weighted average number of common shares outstanding used in the denominator for basic earnings per common share is increased to determine the denominator used for diluted earnings per common share by the effect of potentially dilutive common share equivalents utilizing the treasury stock method. For each of the years ended December 31, 2020 and 2019 common stock equivalents reflected in the table above were related entirely to the incremental shares of unvested restricted stock. The following table presents the calculation of both basic and diluted earnings per share of common stock for the years ended December 31, 2020 and 2019:\n",
      "Note 18. OTHER COMPREHENSIVE INCOME\n",
      "The following tables summarize the reclassifications out of accumulated other comprehensive income for the years ended December 31, 2020 and 2019. The following table summarizes the changes in accumulated other comprehensive income (loss), net of tax, for the years ended December 31, 2020 and 2019:\n",
      "Note 19. CONDENSED FINANCIAL INFORMATION - PARENT COMPANY ONLY\n",
      "The following tables present condensed parent company only financial information:\n",
      "Condensed Statements of Financial Condition\n",
      "Condensed Statements of Income\n",
      "Condensed Statements of Cash Flows\n",
      "Note 20. SELECTED QUARTERLY FINANCIAL DATA (UNAUDITED)\n",
      "Item 9.\n",
      "================================\n",
      "\n",
      "=== Validation Chunk #2 ===\n",
      "Year: 2019\n",
      "Chunk Column: section_1A_chunk_1\n",
      "\n",
      "\n",
      "Question: What are the main credit-related risks faced by FNCB?\n",
      "Answer: FNCB faces various credit-related risks, including the risk that borrowers may not repay loans timely or at all, the potential insufficient value of collateral supporting loans, and the possibility of an increase in nonperforming loans, charge-offs, and delinquencies. These risks are influenced by factors such as local market conditions, general economic conditions, loan underwriting risks, and changes in economic and industry conditions.\n",
      "\n",
      "\n",
      "---Text ---\n",
      "Item 1A. Risk Factors\n",
      "The operations and financial results of FNCB are subject to various risks and uncertainties, including those described below. The risks and uncertainties described below are not the only ones FNCB faces. Additional risks and uncertainties FNCB is unaware of, or currently believes are not material, may also become important factors affecting FNCB. If any of the following risks occur, FNCB’s business, financial condition, operating results and prospects could be materially and adversely affected. In that event, the price of the FNCB’s common stock could decline. Risks Related to FNCB’s Business\n",
      "FNCB is subject to credit risk, which could adversely affect its profitability. FNCB’s business depends on its ability to successfully measure and manage credit risk. As a lender, FNCB is exposed to the risk that the principal of, or interest on, a loan will not be paid timely or at all or that the value of any collateral supporting a loan will be insufficient to cover FNCB’s outstanding exposure. In addition, FNCB is exposed to risks with respect to the period of time over which the loan may be repaid, risks relating to loan underwriting, risks resulting from changes in economic and industry conditions, and risks inherent in dealing with individual loans and borrowers. The creditworthiness of a borrower is affected by many factors including local market conditions and general economic conditions. If the overall economic climate in the United States generally, or in the market areas specifically, experiences material disruption, FNCB’s borrowers may experience difficulties in repaying their loans, the collateral FNCB holds may decrease in value or become illiquid, and FNCB’s level of nonperforming loans, charge-offs and delinquencies could rise and require significant additional provisions for loan losses.\n",
      "================================\n",
      "\n",
      "=== Validation Chunk #3 ===\n",
      "Year: 2018\n",
      "Chunk Column: section_8_chunk_28\n",
      "\n",
      "\n",
      "Question: What is the impact of adopting ASU 2016-01 and ASU 2016-15 on FNCB's financial statements?\n",
      "Answer: The adoption of ASU 2016-01 refined the calculation for determining the fair value of loans held for investment, but did not have a significant impact on FNCB's fair value disclosures. The adoption of ASU 2016-15 had no effect on FNCB's statement of cash flows.\n",
      "\n",
      "\n",
      "---Text ---\n",
      "ASU 2016-01 also requires the use of exit prices to measure fair value of financial instruments. Accordingly, we refined the calculation used to determine the disclosed fair value of FNCB’s loans held for investment as part of adopting this standard. The refined calculation did not have a significant impact on FNCB’s fair value disclosures. For more information about fair value disclosures, refer to Note 15, “Fair Value Measurements” to these consolidated financial statements. ASU 2016-15, Statement of Cash Flows (Topic 230): “Classification of Certain Cash Receipts and Cash Payments,” provides guidance on eight specific cash flow issues in order to reduce current and potential future diversity in reporting. The specific cash flow items addressed include debt prepayment or debt extinguishment costs, settlement of zero-coupon debt instruments with coupon interest rates that are insignificant in relation to the effective interest rate of the borrowing, contingent consideration payments made after a business combination, proceeds from the settlement of insurance claims, proceeds from the settlement of corporate-owned life insurance policies, including bank-owned life insurance policies, distributions received from equity method investees, beneficial interest in securitization transactions, and separately identifiable cash flows and application of the predominance principle. ASU 2016-15 is effective for public business entities for fiscal years beginning after December 15, 2017, and interim periods within those fiscal years. The adoption of this guidance on January 1, 2018 had no effect on the statement of cash flows of FNCB. ASU 2017-09, Compensation - Stock Compensation (Topic 718): “Scope of Modification Accounting” clarifies when it is appropriate to apply modification accounting guidance when there is a change to the terms or conditions of a share-based payment award.\n",
      "================================\n",
      "\n",
      "=== Validation Chunk #4 ===\n",
      "Year: 2019\n",
      "Chunk Column: section_7_chunk_18\n",
      "\n",
      "\n",
      "Question: What factors contributed to the increase in interest expense for FNCB in 2019?\n",
      "Answer: The increase in interest expense was primarily due to higher funding costs. Specifically, rates on interest-bearing deposits increased, with significant rises in both interest-bearing demand deposits and time deposits. Additionally, the rate paid on borrowed funds also increased, all of which contributed to the overall rise in interest expense.\n",
      "\n",
      "\n",
      "---Text ---\n",
      "Despite the slight decline in tax-equivalent net interest income, FNCB’s tax-equivalent interest margin increased 5 basis points to 3.27% in 2019 from 3.22% in 2018. Tax-equivalent net interest margin, a key measurement used in the banking industry to measure income from earning assets relative to the cost to fund those assets, is calculated by dividing tax-equivalent net interest income by average interest-earning assets. Additionally, rate spread, the difference between the average yield on interest-earning assets shown on a fully tax-equivalent basis and the average cost of interest-bearing liabilities, was stable at 3.07% for the years ended December 31, 2019 and 2018. An increase in funding costs was the driving factor leading to the increase in interest expense of $1.2 million, or 14.2%, to $9.8 million in 2019 from $8.6 million in 2018. Rates paid on interest-bearing deposits increased 25 basis points to 0.96% in 2019 from 0.71% in 2018. The increase in deposit rates was concentrated in interest-bearing demand deposits, which increased 24 basis points to 0.81% in 2019 as compared to 0.57% in 2018, and time deposits, which increased 37 basis points to 1.60% in 2019 as compared to 1.23% in 2018. The increases in rates on interest-bearing demand and time deposits contributed $1.2 million and $0.9 million, respectively, to the increase in interest expense. The rate paid on savings deposits remained steady at 0.13% for both 2019 and 2018. Additionally, the rate paid on borrowed funds increased 44 basis points to 2.66% in 2019 from 2.22% in 2018, contributing $0.5 million to the overall increase in interest expense. Partially offsetting the increase to interest expense due to the increase in funding costs was a decrease in volumes of interest-bearing liabilities.\n",
      "================================\n",
      "\n",
      "=== Validation Chunk #5 ===\n",
      "Year: 2018\n",
      "Chunk Column: section_1A_chunk_55\n",
      "\n",
      "\n",
      "Question: What risks are associated with FNCB's internal controls and accounting standards changes?\n",
      "Answer: The risks include potential material misstatements due to errors or fraud, ineffective controls that could adversely affect financial condition and operations, and the impact of new accounting standards such as CECL and lease reporting requirements which may affect capital, regulatory ratios, lending ability, earnings, and performance metrics.\n",
      "\n",
      "\n",
      "---Text ---\n",
      "These controls may not achieve their intended objectives. Control processes that involve human diligence and compliance, such as its disclosure controls and procedures and internal controls over financial reporting, are subject to lapses in judgment and breakdowns resulting from human failures. Controls can also be circumvented by collusion or improper management override. Because of such limitations, there are risks that material misstatements due to error or fraud may not be prevented or detected and that information may not be reported on a timely basis. If FNCB’s controls are not effective, it could have a material adverse effect on its financial condition, results of operations, and market for its common stock, and could subject it to additional regulatory scrutiny. Changes in accounting standards could impact reported earnings. From time to time there are changes in the financial accounting and reporting standards that govern the preparation of financial statements. These changes can materially impact how FNCB records and reports its financial condition and results of operations. In some instances, FNCB could be required to apply a new or revised standard retroactively, resulting in the restatement of prior period financial statements. Changes which have been approved for future implementation, or which are currently proposed or expected to be proposed or adopted include requirements that we: (i) calculate the allowance for loan losses on the basis of the current expected credit losses over the lifetime of our loans, referred to as the CECL model, which is expected to be applicable to us beginning in 2020; and (ii) record the value of and liabilities relating to operating leases on our balance sheet, which is expected to be applicable beginning in 2019. These changes could adversely affect our capital, regulatory capital ratios, ability to make larger loans, earnings and performance metrics.\n",
      "================================\n",
      "\n",
      "=== Validation Chunk #6 ===\n",
      "Year: 2018\n",
      "Chunk Column: section_1A_chunk_57\n",
      "\n",
      "\n",
      "Question: What are the potential impacts of the anti-takeover provisions in FNCB’s charter documents on shareholders and the company?\n",
      "Answer: The anti-takeover provisions in FNCB’s charter documents could discourage, delay, or prevent a change of control, diminish the value of common stock, make it difficult for shareholders to change the board composition, and prevent mergers or acquisitions that shareholders might consider favorable. Specific provisions include staggered board terms, preferred share issuance without shareholder approval, prohibition of cumulative voting, and high shareholder approval thresholds for certain transactions.\n",
      "\n",
      "\n",
      "---Text ---\n",
      "We are evaluating the impact the CECL accounting model will have on our accounting, but expect to recognize a one-time cumulative-effect adjustment to the allowance for loan losses as of the beginning of the first reporting period in which the new standard is effective. We cannot yet determine the magnitude of any such one-time cumulative adjustment or of the overall impact of the new standard on our financial condition or results of operations. Anti-takeover provisions in FNCB’s charter documents could discourage, delay or prevent a change of control of FNCB’s company and diminish the value of FNCB’s common stock. Some of the provisions of FNCB’s amended and restated articles of incorporation, as amended, and amended and restated bylaws, as amended, could make it difficult for its shareholders to change the composition of its board of directors, preventing them from changing the composition of management. In addition, the same provisions may discourage, delay or prevent a merger or acquisition that FNCB’s shareholders may consider favorable. These provisions include:\n",
      "●\n",
      "classifying FNCB’s board of directors into three classes of directors with staggered three-year terms; ●\n",
      "authorizing FNCB’s board of directors to issue preferred shares without shareholder approval; ●\n",
      "prohibiting cumulative voting in the election of directors; ●\n",
      "requiring the approval of 75% of FNCB’s shareholders to approve any merger or sale of all, or substantially all, unless approval of such proposed transaction is recommended by at least a majority of FNCB’s entire board of directors; ●\n",
      "authorizing FNCB’s board of directors to, if it deems advisable, oppose a tender or other offer for FNCB’s securities; and\n",
      "●\n",
      "requiring the approval of 75% of FNCB’s shareholders to amend certain provisions relating to business combinations not approved by the board of directors.\n",
      "================================\n",
      "\n",
      "=== Validation Chunk #7 ===\n",
      "Year: 2020\n",
      "Chunk Column: section_7_chunk_5\n",
      "\n",
      "\n",
      "Question: How does FNCB determine the fair value of its investment portfolio?\n",
      "Answer: FNCB determines the fair value of its investment portfolio using various inputs, including unadjusted quoted market prices in active markets (Level 1), quoted prices for similar assets or models with observable inputs (Level 2), and valuation techniques based on unobservable and significant inputs (Level 3). For Level 3 inputs, management uses assumptions such as cash flows, discount rates, adjustments for nonperformance and liquidity, and liquidation values. A significant degree of judgment is involved in valuing investments using Level 3 inputs, which could positively or negatively affect FNCB’s financial condition or results of operations.\n",
      "\n",
      "\n",
      "---Text ---\n",
      "See Note 2, “Summary of Significant Accounting Policies” and Note 5, “Loans” of the notes to consolidated financial statements included in Item 8, “Financial Statements and Supplementary Data” to this Annual Report on Form 10-K for additional information about the ALLL. Securities Valuation and Evaluation for Impairment\n",
      "Management utilizes various inputs to determine the fair value of its investment portfolio. To the extent they exist, unadjusted quoted market prices in active markets (Level 1) or quoted prices for similar assets or models using inputs that are observable, either directly or indirectly (Level 2) are utilized to determine the fair value of each investment in the portfolio. In the absence of observable inputs or if markets are illiquid, valuation techniques are used to determine fair value of any investments that require inputs that are both unobservable and significant to the fair value measurement (Level 3). For Level 3 inputs, valuation techniques are based on various assumptions, including, but not limited to, cash flows, discount rates, adjustments for nonperformance and liquidity, and liquidation values. A significant degree of judgment is involved in valuing investments using Level 3 inputs. The use of different assumptions could have a positive or negative effect on FNCB’s financial condition or results of operations. See Note 4, “Securities” and Note 15, “Fair Value Measurements” of the notes to consolidated financial statements included in Item 8, “Financial Statements and Supplementary Data” to this Annual Report on Form 10-K for additional information about FNCB’s securities valuation techniques. On a quarterly basis, management evaluates individual investment securities in an unrealized loss position for other than temporary impairment (“OTTI”).\n",
      "================================\n",
      "\n",
      "=== Validation Chunk #8 ===\n",
      "Year: 2018\n",
      "Chunk Column: section_8_chunk_52\n",
      "\n",
      "\n",
      "Question: What is FNCB's maximum borrowing capacity under federal funds lines of credit and the Federal Reserve Discount Window as of December 31, 2018?\n",
      "Answer: $40.0 million and $9.9 million, respectively.\n",
      "\n",
      "\n",
      "---Text ---\n",
      "FNCB’s maximum borrowing capacity under federal funds lines of credit and the Federal Reserve Discount Window was $40.0 million and $9.9 million, respectively at December 31, 2018. Federal funds lines of credit are unsecured, while any borrowings through the Federal Reserve Discount Window are fully collateralized by certain pledged loans in the amount of $17.9 million at December 31, 2018. FNCB has an agreement with the FHLB of Pittsburgh which allows for borrowings, either overnight or term, up to its maximum borrowing capacity, which is based on a percentage of qualifying loans pledged under a blanket pledge agreement. Loans of $492.3 million and $448.2 million, at December 31, 2018 and 2017, respectively, were pledged to collateralize borrowings under this agreement. FNCB’s maximum borrowing capacity was $344.9 million at December 31, 2018, of which $12.3 million in fixed-rate advances having original maturities between two years and five years, $6.6 million in overnight funds and $47.5 million in letters of credit to secure municipal deposits, were outstanding. In addition to pledging loans, FNCB is required to purchase FHLB of Pittsburgh stock based upon the amount of advances and letters of credit outstanding. On December 14, 2006, the Issuing Trust issued $10.0 million of trust preferred securities (the “Trust Securities”) at a variable interest rate of 7.02%, with a scheduled maturity of December 15, 2036. FNCB owns 100.0% of the ownership interest in the Issuing Trust. The proceeds from the issue were invested in $10.3 million, 7.02% Junior Subordinated Debentures (the “Debentures”) issued by FNCB. The interest rate on the Trust Securities and the Debentures resets quarterly at a spread of 1.67% above the current 3-month LIBOR rate. The average interest rate paid on the Debentures was 3.88% in 2018 and 2.90% in 2017.\n",
      "================================\n",
      "\n",
      "=== Validation Chunk #9 ===\n",
      "Year: 2020\n",
      "Chunk Column: section_8_chunk_5\n",
      "\n",
      "\n",
      "Question: What is the primary business of FNCB Bancorp, Inc. and its subsidiaries?\n",
      "Answer: FNCB Bancorp, Inc. is a registered bank holding company that provides customary retail and commercial banking services to individuals, businesses, and local governments and municipalities through its 17 full-service branch locations in Northeastern Pennsylvania.\n",
      "\n",
      "\n",
      "---Text ---\n",
      "and,\n",
      "o Recalculation of the allowance for loan loss and allocation of qualitative factor adjustments to the appropriate loan segments. /s/ Baker Tilly US, LLP\n",
      "We have served as the Company’s auditor since 2014. Baker Tilly US, LLP (formerly known as Baker Tilly Virchow Krause, LLP)\n",
      "Iselin, New Jersey\n",
      "March 12, 2021\n",
      "FNCB BANCORP, INC. AND SUBSIDIARIES\n",
      "CONSOLIDATED STATEMENTS OF FINANCIAL CONDITION\n",
      "The accompanying notes to consolidated financial statements are an integral part of these statements. FNCB BANCORP, INC. AND SUBSIDIARIES\n",
      "CONSOLIDATED STATEMENTS OF INCOME\n",
      "The accompanying notes to consolidated financial statements are an integral part of these statements. FNCB BANCORP, INC. AND SUBSIDIARIES\n",
      "CONSOLIDATED STATEMENTS OF COMPREHENSIVE INCOME\n",
      "The accompanying notes to consolidated financial statements are an integral part of these statements. The accompanying notes to consolidated financial statements are an integral part of these statements. FNCB BANCORP, INC AND SUBSIDIARIES\n",
      "CONSOLIDATED STATEMENTS OF CASH FLOWS\n",
      "The accompanying notes to consolidated financial statements are an integral part of these statements. Notes to Consolidated Financial Statements\n",
      "Note 1. ORGANIZATION\n",
      "FNCB Bancorp, Inc. is a registered bank holding company under the Bank Holding Company Act of 1956, incorporated under the laws of the Commonwealth of Pennsylvania in 1997. It is the parent company of FNCB Bank (the “Bank”) and the Bank’s wholly owned subsidiaries FNCB Realty Company, Inc., FNCB Realty Company I, LLC, and FNCB Realty Company II, LLC. Unless the context otherwise requires, the term “FNCB” is used to refer to FNCB Bancorp, Inc., and its subsidiaries. In certain circumstances, however, the term “FNCB” refers to FNCB Bancorp, Inc., itself. The Bank provides customary retail and commercial banking services to individuals, businesses and local governments and municipalities through its 17 full-service branch locations within its primary market area, Northeastern Pennsylvania.\n",
      "================================\n",
      "\n",
      "=== Validation Chunk #10 ===\n",
      "Year: 2019\n",
      "Chunk Column: section_1A_chunk_54\n",
      "\n",
      "\n",
      "Question: What are the regulatory and compliance challenges FNCB faces as a public company?\n",
      "Answer: FNCB, as a public company, is subject to various regulations such as the Exchange Act, Sarbanes-Oxley Act, FDIC regulations, and SEC's proxy rules. These regulations require FNCB to file periodic reports, maintain effective disclosure controls and internal financial reporting controls, and prepare compliant proxy materials. Compliance with these rules increases legal and financial costs, complicates certain activities, consumes more time and resources, and creates additional demands on FNCB’s systems.\n",
      "\n",
      "\n",
      "---Text ---\n",
      "(b) any person not otherwise defined as a company by the BHC Act and its implementing regulations may be required to obtain the approval of the Federal Reserve under the Change in Bank Control Act to acquire or retain 10% or more of FNCB’s outstanding securities. Becoming a bank holding company imposes statutory and regulatory restrictions and obligations, such as providing managerial and financial strength for its bank subsidiaries. Regulation as a bank holding company could require the holder to divest all or a portion of the holder’s investment in FNCB’s securities or those nonbanking investments that may be deemed impermissible or incompatible with bank holding company status, such as a material investment in a company unrelated to banking. The requirements of being a public company may strain FNCB’s resources and divert management's attention. FNCB is a public company, subject to the reporting requirements of the Exchange Act, the Sarbanes-Oxley Act and applicable securities rules and regulations. Under FDIC regulations, the Sarbanes-Oxley Act and regulations increase the scope, complexity and cost of corporate governance, reporting and disclosure practices over those of non-public or non-reporting companies. Among other things, the Exchange Act requires that FNCB file annual, quarterly and current reports with respect to its business and operating results and maintain effective disclosure controls and procedures and internal control over financial reporting. As a Nasdaq listed company, FNCB is also required to prepare and file proxy materials which meet the requirements of the Exchange Act and the SEC's proxy rules. Compliance with these rules and regulations increase FNCB’s legal and financial compliance costs, make some activities more difficult, time-consuming or costly, and increase demand on FNCB’s systems and resources, particularly if FNCB becomes ineligible to report as a “smaller reporting company” as defined in the SEC’s regulations.\n",
      "================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Display and store the validation dataset\n",
    "\n",
    "# Step 3.1: Just print them directly (as Python dicts)\n",
    "for i, item in enumerate(validation_data, start=1):\n",
    "    print(f\"=== Validation Chunk #{i} ===\")\n",
    "    print(\"Year:\", item[\"year\"])\n",
    "    print(\"Chunk Column:\", item[\"chunk_col_name\"])\n",
    "    print(\"\\n\")\n",
    "    print(\"Question:\", item[\"question\"])\n",
    "    print(\"Answer:\", item[\"answer\"])\n",
    "    print(\"\\n\")\n",
    "    print(\"---Text ---\")\n",
    "    print(item[\"text\"])\n",
    "    print(\"================================\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "91be2b52-439c-4def-adfb-baf4c2ebed40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: What was the trend in the company's tax-equivalent net interest margin and rate spread in 2019 compared to 2018, and what were the key factors contributing to these changes?\n",
      "A: The company's tax-equivalent net interest margin increased by 5 basis points to 3.27% in 2019 from 3.22% in 2018. The rate spread remained stable at 3.07% for both years. The increase in the net interest margin was driven by a rise in funding costs, particularly an increase in rates paid on interest-bearing deposits and borrowed funds. Specifically, rates on interest-bearing demand deposits increased by 24 basis points, time deposits by 37 basis points, and borrowed funds by 44 basis points, contributing to higher interest expense.\n",
      "\n",
      "Q: What is the company's stance on potential mergers or acquisitions, and how might this impact its shareholders?\n",
      "A: The company has implemented anti-takeover provisions that could discourage, delay, or prevent mergers or acquisitions. These include requiring 75% shareholder approval for certain transactions unless approved by the board of directors, authorizing the board to oppose tender offers, and classifying the board into three classes with staggered terms. These measures may protect the company's strategic direction but could also diminish shareholder value by limiting opportunities for favorable mergers or acquisitions.\n",
      "\n",
      "Q: What methods does management use to determine the fair value of its investment portfolio?\n",
      "A: Management utilizes various inputs, primarily unadjusted quoted market prices in active markets (Level 1) or observable inputs like quotes for similar assets or model outputs (Level 2). When necessary, especially with illiquid markets or unobservable inputs, valuation techniques based on assumptions such as cash flows and discount rates are applied (Level 3).\n",
      "\n",
      "Q: What is the company's maximum borrowing capacity under different credit facilities as of December 31, 2018?\n",
      "A: As of December 31, 2018, the company's maximum borrowing capacity was $40.0 million under federal funds lines of credit and $9.9 million through the Federal Reserve Discount Window. Additionally, their agreement with the FHLB of Pittsburgh allowed for borrowings up to $344.9 million based on pledged loans.\n",
      "\n",
      "Q: What is the company's organizational structure?\n",
      "A: The company is a registered bank holding company incorporated under Pennsylvania law in 1997 and serves as the parent company to a bank and its subsidiaries, which include realty-related entities. The term 'the company' refers to both the holding company and its consolidated subsidiaries unless otherwise specified. The bank operates several full-service branch locations in its primary market area.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "validation_data_json = \"\"\"\n",
    "[\n",
    "  {\n",
    "    \"question\": \"What was the trend in the company's tax-equivalent net interest margin and rate spread in 2019 compared to 2018, and what were the key factors contributing to these changes?\",\n",
    "    \"answer\": \"The company's tax-equivalent net interest margin increased by 5 basis points to 3.27% in 2019 from 3.22% in 2018. The rate spread remained stable at 3.07% for both years. The increase in the net interest margin was driven by a rise in funding costs, particularly an increase in rates paid on interest-bearing deposits and borrowed funds. Specifically, rates on interest-bearing demand deposits increased by 24 basis points, time deposits by 37 basis points, and borrowed funds by 44 basis points, contributing to higher interest expense.\"\n",
    "  },\n",
    "  {\n",
    "    \"question\": \"What is the company's stance on potential mergers or acquisitions, and how might this impact its shareholders?\",\n",
    "    \"answer\": \"The company has implemented anti-takeover provisions that could discourage, delay, or prevent mergers or acquisitions. These include requiring 75% shareholder approval for certain transactions unless approved by the board of directors, authorizing the board to oppose tender offers, and classifying the board into three classes with staggered terms. These measures may protect the company's strategic direction but could also diminish shareholder value by limiting opportunities for favorable mergers or acquisitions.\"\n",
    "  },\n",
    "  {\n",
    "    \"question\": \"What methods does management use to determine the fair value of its investment portfolio?\",\n",
    "    \"answer\": \"Management utilizes various inputs, primarily unadjusted quoted market prices in active markets (Level 1) or observable inputs like quotes for similar assets or model outputs (Level 2). When necessary, especially with illiquid markets or unobservable inputs, valuation techniques based on assumptions such as cash flows and discount rates are applied (Level 3).\"\n",
    "  },\n",
    "  {\n",
    "    \"question\": \"What is the company's maximum borrowing capacity under different credit facilities as of December 31, 2018?\",\n",
    "    \"answer\": \"As of December 31, 2018, the company's maximum borrowing capacity was $40.0 million under federal funds lines of credit and $9.9 million through the Federal Reserve Discount Window. Additionally, their agreement with the FHLB of Pittsburgh allowed for borrowings up to $344.9 million based on pledged loans.\"\n",
    "  },\n",
    "  {\n",
    "    \"question\": \"What is the company's organizational structure?\",\n",
    "    \"answer\": \"The company is a registered bank holding company incorporated under Pennsylvania law in 1997 and serves as the parent company to a bank and its subsidiaries, which include realty-related entities. The term 'the company' refers to both the holding company and its consolidated subsidiaries unless otherwise specified. The bank operates several full-service branch locations in its primary market area.\"\n",
    "  }\n",
    "]\n",
    "\"\"\"\n",
    "\n",
    "validation_data = json.loads(validation_data_json)\n",
    "\n",
    "for qa in validation_data:\n",
    "    print(\"Q:\", qa[\"question\"])\n",
    "    print(\"A:\", qa[\"answer\"])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "268ced59-76a9-4817-b006-e7220123e88b",
   "metadata": {},
   "source": [
    "# Demonstrate that your LLM can retrieve the correct chunks from your embedding object for the correct year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "33697762-e7fc-4bb8-82da-1d5022d32462",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_by_year_and_search(\n",
    "    year: int,\n",
    "    query: str,\n",
    "    top_k: int = 5\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Filters the df_embeddings DataFrame by the specified year, computes the query embedding,\n",
    "    calculates the cosine similarity with the 'embeddings_vector' column, \n",
    "    and selects the top_k most similar chunks. It then concatenates these chunks into a prompt \n",
    "    string following the specified structure:\n",
    "    \n",
    "    Prompt Structure:\n",
    "      1) Role: \"You are a financial analysis assistant...\"\n",
    "      2) '### Question ###' with the user's query\n",
    "      3) '### Context (Top {top_k} Chunks) ###' containing:\n",
    "         - [Chunk Info] (year, chunk_col_name, similarity)\n",
    "         - [Chunk Text]\n",
    "      4) A '### Requirements for Response Format ###' section instructing the model\n",
    "         to respond in JSON format with keys: \"year\", \"financial_factor\", \"answer\", \"reference\".\n",
    "    \n",
    "    Args:\n",
    "        year (int): The year to filter (e.g., 2018).\n",
    "        query (str): The user query (e.g., \"What is the company's net income for 2018?\").\n",
    "        top_k (int, optional): Number of top similar chunks to include (default=5).\n",
    "    \n",
    "    Returns:\n",
    "        str: A single string containing the assembled prompt with context chunks and instructions.\n",
    "    \"\"\"\n",
    "    # We assume df_embeddings is accessible in this scope. If not, pass it as a parameter.\n",
    "    # Also assume embedding_pipeline_model is available globally for get_query_embedding().\n",
    "    from pyspark.sql.functions import col\n",
    "\n",
    "    # 1) Filter the DataFrame by the specified year\n",
    "    #    Note: If your 'year' is stored as an integer in df_embeddings, \n",
    "    #    cast or compare as needed. If it's stored as string, do .isin([str(year)]).\n",
    "    df_filtered = df_embeddings.filter(col(\"year\") == str(year))\n",
    "    \n",
    "    # 2) Compute the query embedding\n",
    "    query_vec = get_query_embedding(query, pipeline_model)\n",
    "    cos_sim_udf = create_cosine_similarity_udf(query_vec)\n",
    "    \n",
    "    # 3) Compute similarity, sort descending, and pick top_k\n",
    "    df_with_sim = df_filtered.withColumn(\"similarity\", cos_sim_udf(col(\"embeddings_vector\")))\n",
    "    df_topk = df_with_sim.orderBy(col(\"similarity\").desc()).limit(top_k)\n",
    "    \n",
    "    # 4) Build context blocks for the top_k chunks\n",
    "    rows = df_topk.collect()\n",
    "    context_blocks = []\n",
    "    for i, r in enumerate(rows, start=1):\n",
    "        block = (\n",
    "            f\"[Chunk Info]\\n\"\n",
    "            f\"- year: {r['year']}\\n\"\n",
    "            f\"- chunk_col_name: {r['chunk_col_name']}\\n\"\n",
    "            f\"- similarity: {r['similarity']:.4f}\\n\\n\"\n",
    "            f\"[Chunk Text]\\n{r['text']}\\n\"\n",
    "            \"----\\n\"\n",
    "        )\n",
    "        context_blocks.append(block)\n",
    "    context_str = \"\\n\".join(context_blocks)\n",
    "    \n",
    "    # 5) Construct the final prompt\n",
    "    prompt_result = f\"\"\"You are a financial analysis assistant.\n",
    "Given the following text from a 10-K filing, please answer the question.\n",
    "\n",
    "### Context (Top {top_k} Chunks) ###\n",
    "\n",
    "{context_str}\n",
    "\n",
    "### Requirements for Response Format ###\n",
    "Your response MUST be valid JSON in the format:\n",
    "{{\n",
    "  \"year\": \"...\",\n",
    "  \"financial_factor\": \"...\",\n",
    "  \"answer\": \"...\",\n",
    "  \"reference\": {{\n",
    "    \"chunk_col_name\": ...\",\n",
    "    \"text_snippet\": \"...\"\n",
    "  }}\n",
    "}}\n",
    "\n",
    "### Question ###\n",
    "{query}\n",
    "\n",
    "If there is no explicit mention in the text, return \"unknow\".\n",
    "\n",
    "\"\"\"\n",
    "    return prompt_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5ad36027-e7b7-4d08-b4e5-c1f54412cf37",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_ollama_with_filter_tool(\n",
    "    query_text: str,\n",
    "    model: str = \"llama3.3\"\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    A single function that:\n",
    "      1) Defines the 'tools' list with filter_by_year_and_search.\n",
    "      2) Calls ollama.chat(...) using the given model (default='llama3.3').\n",
    "      3) Iterates through response.tool_calls to execute the needed function.\n",
    "      4) Returns the final 'result_str' produced by the filter_by_year_and_search tool.\n",
    "    \n",
    "    Args:\n",
    "        query_text (str): The user's query or prompt message.\n",
    "        model (str, optional): The Ollama model name to use. Defaults to 'llama3.3'.\n",
    "        \n",
    "    Returns:\n",
    "        str: The output (prompt or final string) returned by the filter_by_year_and_search tool.\n",
    "    \"\"\"\n",
    "    # 1) Define the tools list (includes your filter_by_year_and_search function)\n",
    "    tools = [filter_by_year_and_search]\n",
    "\n",
    "    # 2) Make the Ollama call, passing the 'tools' argument\n",
    "    response = ollama.chat(\n",
    "        model=model,\n",
    "        messages=[{'role': 'user', 'content': query_text}],\n",
    "        tools=tools,\n",
    "    )\n",
    "\n",
    "    # 3) Prepare a lookup dict for function references\n",
    "    available_functions = {\n",
    "        'filter_by_year_and_search': filter_by_year_and_search,\n",
    "    }\n",
    "\n",
    "    # 4) Iterate over any tool calls that the LLM made\n",
    "    result_str = None\n",
    "    for tool_call in response.message.tool_calls or []:\n",
    "        func = available_functions.get(tool_call.function.name)\n",
    "        if func:\n",
    "            # We assume the LLM will supply correct arguments. \n",
    "            # For example: {'year': 2018, 'query': \"...\", 'top_k': 5}\n",
    "            result_str = func(**tool_call.function.arguments)\n",
    "            print(result_str)\n",
    "        else:\n",
    "            print(\"Function not found:\", tool_call.function.name)\n",
    "    \n",
    "    return result_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd7365df-236a-463b-8851-99e56e27c37a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c72617d1-7184-4793-9963-ebbe707ba1b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "75fe614f-4fcc-4a4c-944d-84870cfd3074",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is the company's maximum borrowing capacity under different credit facilities as of December 31, 2018?\n"
     ]
    }
   ],
   "source": [
    "query_text = validation_data[3][\"question\"]\n",
    "print(query_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "701b6cea-a706-46f8-b220-25b4d3d4993b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "As of December 31, 2018, the company's maximum borrowing capacity was $40.0 million under federal funds lines of credit and $9.9 million through the Federal Reserve Discount Window. Additionally, their agreement with the FHLB of Pittsburgh allowed for borrowings up to $344.9 million based on pledged loans.\n"
     ]
    }
   ],
   "source": [
    "print(validation_data[3][\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c7841e52-9653-4a5f-937c-efba6085b18f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are a financial analysis assistant.\n",
      "Given the following text from a 10-K filing, please answer the question.\n",
      "\n",
      "### Context (Top 5 Chunks) ###\n",
      "\n",
      "[Chunk Info]\n",
      "- year: 2018\n",
      "- chunk_col_name: section_7_chunk_12\n",
      "- similarity: 0.8668\n",
      "\n",
      "[Chunk Text]\n",
      "FNCB paid dividends to holders of common stock of $0.17 per share and $0.13 per share for the years ended December 31, 2018 and 2017, respectively. Balance Sheet Profile\n",
      "Total assets increased $75.4 million, or 6.5%, to $1.238 billion at December 31, 2018 from $1.162 billion at December 31, 2017. The increase in total assets primarily reflected strong growth in interest-earning assets. Specifically, loans, net of net deferred costs and unearned income, increased $68.5 million, or 8.9%, to $839.1 million at December 31, 2018 from $770.6 million at December 31, 2017. In addition, securities available for sale increased $6.5 million, or 2.3%, to $296.0 million at December 31, 2018 from $289.5 million at the end of 2017. The asset growth was funded with an increase in total deposits of $93.2 million, or 9.3%, to $1.096 billion at December 31, 2018 from $1.002 billion at December 31, 2017. The increase in deposits was primarily attributable to increases in retail and wholesale time deposits. Borrowings through the Federal Home Loan Bank of Pittsburgh decreased $26.0 million, or 57.9%, to $18.9 million at December 31, 2018 from $44.9 million at December 31, 2017. Total shareholders’ equity increased $8.0 million, or 9.0%, to $97.2 million at December 31, 2018 from $89.2 million at the end of 2017, which resulted primarily from net income in 2018 of $13.3 million, partially offset by a $2.8 million increase in accumulated other comprehensive loss related to depreciation in the fair value of available-for-sale debt securities, net of deferred taxes, and year-to-date dividends declared of $2.9 million. At December 31, 2018, FNCB’s total risk-based capital ratio and the Tier 1 leverage ratio were 12.69% and 8.50%, respectively. The respective ratios for the Bank at December 31, 2018 were 12.17% and 8.27%.\n",
      "----\n",
      "\n",
      "[Chunk Info]\n",
      "- year: 2018\n",
      "- chunk_col_name: section_8_chunk_51\n",
      "- similarity: 0.8644\n",
      "\n",
      "[Chunk Text]\n",
      "The property is currently owned by the Bank and houses a separate drive-thru location, as well as a drive-thru and a walk-up ATM. The project is anticipated to cost $2.0 million and will be funded by cash generated through operations. FNCB incurred a $148 thousand abandonment charge related to the existing drive-thru location which is included in other losses in the consolidated statements of income for the year ended December 31, 2018. Note 7. DEPOSITS\n",
      "The following table summarizes deposits by major category at December 31, 2018 and 2017:\n",
      "The aggregate amount of deposits reclassified as loans was $36 thousand at December 31, 2018 and $57 thousand at December 31, 2017. Management evaluates transaction accounts that are overdrawn for collectability as part of its evaluation for credit losses. During 2018 and 2017, no deposits were received on terms other than those available in the normal course of business. The following table summarizes scheduled maturities of time deposits, including certificates of deposit and individual retirement accounts, at December 31, 2018:\n",
      "Investment securities with a carrying value of $286.4 million and $282.3 million at December 31, 2018 and 2017, respectively, were pledged to collateralize certain municipal deposits. In addition, FNCB had outstanding letters of credit with the FHLB to secure municipal deposits of $47.5 million and $5.0 million at December 31, 2018 and 2017, respectively. Note 8. BORROWED FUNDS/SUBSEQUENT EVENT\n",
      "Short-term borrowings available to FNCB include overnight FHLB of Pittsburgh advances, federal funds lines of credit and the Federal Reserve Discount Window, which generally represent overnight or less than 30-day borrowings. FNCB’s maximum borrowing capacity under federal funds lines of credit and the Federal Reserve Discount Window was $40.0 million and $9.9 million, respectively at December 31, 2018.\n",
      "----\n",
      "\n",
      "[Chunk Info]\n",
      "- year: 2018\n",
      "- chunk_col_name: section_7_chunk_27\n",
      "- similarity: 0.8627\n",
      "\n",
      "[Chunk Text]\n",
      "These determinations are inherently subjective and depend upon management’s estimates and judgments used in their evaluation of both positive and negative evidence. In evaluating available evidence, management considers, among other factors, historical financial performance, expectation of future earnings, the ability to carry back losses to recoup taxes previously paid, length of statutory carry forward periods, experience with operating loss and tax credit carry forwards not expiring unused, tax planning strategies and timing of reversals of temporary differences. In assessing the need for a valuation allowance, management carefully weighs both positive and negative evidence currently available. Management performed an evaluation of FNCB’s deferred tax assets at December 31, 2018 taking into consideration both positive and negative evidence as of that date. Based on this evaluation, management believes that FNCB's future taxable income will be sufficient to utilize deferred tax assets. Accordingly, management concluded that no valuation allowance for deferred tax assets was required at December 31, 2018 and 2017. FINANCIAL CONDITION\n",
      "Total assets were $1.238 billion at December 31, 2018, an increase of $75.4 million, or 6.5%, from $1.162 billion at December 31, 2017. The increase in total assets was driven by growth in interest-earning assets, specifically an increase in loans, net of net deferred loan costs and unearned income of $68.5 million, or 8.9%, to $839.1 million at December 31, 2018 from $770.6 million at December 31, 2017. In addition, available-for-sale debt securities increased $6.5 million, or 2.3% to $296.0 million at December 31, 2018 from $289.5 million at December 31, 2017. The asset growth was funded with an increase in total deposits of $93.2 million, or 9.3%, to $1.096 billion at December 31, 2018 from $1.002 billion at December 31, 2017. The increase in deposits was primarily attributable to increases in retail and wholesale time deposits.\n",
      "----\n",
      "\n",
      "[Chunk Info]\n",
      "- year: 2018\n",
      "- chunk_col_name: section_7_chunk_53\n",
      "- similarity: 0.8596\n",
      "\n",
      "[Chunk Text]\n",
      "The increase in rate on the long-term debt was due to increases in the rates paid on FHLB borrowings and junior subordinated debentures, which directly correlated with the increases in market interest rates throughout 2018. The maximum amount of total borrowings outstanding at any month end during the years ended December 31, 2018 and 2017 were $203.2 million and $97.2 million, respectively. For further discussion of FNCB’s borrowings, see Note 8, “Borrowed Funds” in the Notes to the consolidated financial statements included in Item 8 of this Annual Report on Form 10-K. Liquidity\n",
      "The term liquidity refers to the ability to generate sufficient amounts of cash to meet cash flow needs. Liquidity is required to fulfill the borrowing needs of FNCB’s credit customers and the withdrawal and maturity requirements of its deposit customers, as well as to meet other financial commitments. FNCB’s liquidity position is impacted by several factors, which include, among others, loan origination volumes, loan and investment maturity structure and cash flows, deposit demand and time deposit maturity structure and retention. FNCB has liquidity and contingent funding policies in place that are designed with controls in place to provide advanced detection of potentially significant funding shortfalls, establish methods for assessing and monitoring risk levels, and institute prompt responses that may alleviate a potential liquidity crisis. Management monitors FNCB’s liquidity position and fluctuations daily, forecasts future liquidity needs, performs periodic stress tests on its liquidity levels and develops strategies to ensure adequate liquidity at all times. The statements of cash flows present the change in cash and cash equivalents from operating, investing and financing activities. Cash and due from banks and interest-bearing deposits in other banks, which comprise cash and cash equivalents, are FNCB’s most liquid assets.\n",
      "----\n",
      "\n",
      "[Chunk Info]\n",
      "- year: 2018\n",
      "- chunk_col_name: section_8_chunk_52\n",
      "- similarity: 0.8585\n",
      "\n",
      "[Chunk Text]\n",
      "FNCB’s maximum borrowing capacity under federal funds lines of credit and the Federal Reserve Discount Window was $40.0 million and $9.9 million, respectively at December 31, 2018. Federal funds lines of credit are unsecured, while any borrowings through the Federal Reserve Discount Window are fully collateralized by certain pledged loans in the amount of $17.9 million at December 31, 2018. FNCB has an agreement with the FHLB of Pittsburgh which allows for borrowings, either overnight or term, up to its maximum borrowing capacity, which is based on a percentage of qualifying loans pledged under a blanket pledge agreement. Loans of $492.3 million and $448.2 million, at December 31, 2018 and 2017, respectively, were pledged to collateralize borrowings under this agreement. FNCB’s maximum borrowing capacity was $344.9 million at December 31, 2018, of which $12.3 million in fixed-rate advances having original maturities between two years and five years, $6.6 million in overnight funds and $47.5 million in letters of credit to secure municipal deposits, were outstanding. In addition to pledging loans, FNCB is required to purchase FHLB of Pittsburgh stock based upon the amount of advances and letters of credit outstanding. On December 14, 2006, the Issuing Trust issued $10.0 million of trust preferred securities (the “Trust Securities”) at a variable interest rate of 7.02%, with a scheduled maturity of December 15, 2036. FNCB owns 100.0% of the ownership interest in the Issuing Trust. The proceeds from the issue were invested in $10.3 million, 7.02% Junior Subordinated Debentures (the “Debentures”) issued by FNCB. The interest rate on the Trust Securities and the Debentures resets quarterly at a spread of 1.67% above the current 3-month LIBOR rate. The average interest rate paid on the Debentures was 3.88% in 2018 and 2.90% in 2017.\n",
      "----\n",
      "\n",
      "\n",
      "### Requirements for Response Format ###\n",
      "Your response MUST be valid JSON in the format:\n",
      "{\n",
      "  \"year\": \"...\",\n",
      "  \"financial_factor\": \"...\",\n",
      "  \"answer\": \"...\",\n",
      "  \"reference\": {\n",
      "    \"chunk_col_name\": ...\",\n",
      "    \"text_snippet\": \"...\"\n",
      "  }\n",
      "}\n",
      "\n",
      "### Question ###\n",
      "What is the company's maximum borrowing capacity under different credit facilities as of December 31, 2018?\n",
      "\n",
      "If there is no explicit mention in the text, return \"unknow\".\n",
      "\n",
      "\n",
      "<think>\n",
      "Alright, I need to figure out the company's maximum borrowing capacity under different credit facilities as of December 31, 2018. Let me go through each chunk provided and look for relevant information.\n",
      "\n",
      "First, looking at the first chunk, it mentions FHLB borrowings but doesn't specify the maximum capacity. The second chunk talks about deposits and liquidity but not directly about borrowing limits. \n",
      "\n",
      "The third chunk discusses long-term debt rates and mentions that the maximum total borrowings were $203.2 million in 2018, but this seems to be the actual amount borrowed, not the capacity.\n",
      "\n",
      "Moving on to the fourth chunk, it states FNCB's maximum borrowing capacity under federal funds lines of credit was $40 million and $9.9 million for the Federal Reserve Discount Window as of December 31, 2018. Additionally, there's a mention of an agreement with FHLB allowing up to $344.9 million based on pledged loans.\n",
      "\n",
      "So, from this chunk, I can extract that under federal funds lines it's $40 million and under the Federal Reserve Discount Window it's $9.9 million. The FHLB agreement allows for up to $344.9 million.\n",
      "</think>\n",
      "\n",
      "The company's maximum borrowing capacity under different credit facilities as of December 31, 2018, includes $40 million under federal funds lines of credit, $9.9 million through the Federal Reserve Discount Window, and $344.9 million via an FHLB agreement.\n",
      "\n",
      "```json\n",
      "{\n",
      "  \"year\": \"2018\",\n",
      "  \"financial_factor\": \"Maximum Borrowing Capacity\",\n",
      "  \"answer\": \"$40.0 million under federal funds lines of credit, $9.9 million through the Federal Reserve Discount Window, and $344.9 million under the FHLB agreement.\",\n",
      "  \"reference\": {\n",
      "    \"chunk_col_name\": \"section_8_chunk_52\",\n",
      "    \"text_snippet\": \"FNCB’s maximum borrowing capacity under federal funds lines of credit and the Federal Reserve Discount Window was $40.0 million and $9.9 million, respectively at December 31, 2018. [...] FNCB’s maximum borrowing capacity was $344.9 million at December 31, 2018...\"\n",
      "  }\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "response: ChatResponse = chat(model='deepseek-r1:70b', \n",
    "                              messages=[{'role': 'user','content': run_ollama_with_filter_tool(query_text)}],\n",
    "                              options={\"temperature\":0})\n",
    "\n",
    "print(response.message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ff20953-f5fa-46d6-b00b-cf7782f29731",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13d672b2-723c-4b1b-92b5-4232c7f11c2c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0d13d187-94cc-4646-af5a-6481b7e3e6eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop Spark\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b19bbcc-73fa-4c9a-bd01-6fc5391bbf3b",
   "metadata": {},
   "source": [
    "# Summary\n",
    "\n",
    "In this task, I reused some utility functions from Task 1.\n",
    "\n",
    "## Key Objective  \n",
    "The main goal of this task is to **demonstrate that the LLM can retrieve the correct chunks from the embedding object for the specified year**. This involves two key aspects:  \n",
    "1. **Retrieving the correct chunk** based on the query.  \n",
    "2. **Identifying the correct year from the user query.**  \n",
    "\n",
    "For the second aspect, I adopted a **function call approach**, where a **customized function** is used. The LLM automatically extracts the year from the query and passes it as an argument to the function, rather than relying on **hard-coded logic** or **NER-based methods**.\n",
    "\n",
    "## Scalability Considerations  \n",
    "Given the potential expansion of the dataset to cover **more years, more companies, and more table types**, this function-based approach enables flexible **subset selection** as a core part of the logic.\n",
    "\n",
    "However, this task does not yet explore challenges related to **cross-table and cross-year retrieval**. If needed, an **agent-based approach** for complex query decomposition could be considered.\n",
    "\n",
    "## Dataset Limitations  \n",
    "A crucial challenge in this task is the **poor parsing quality** of the dataset. Without access to the **original filling forms**, some critical **first-hand information** (such as heading levels and structured table data) is lost.\n",
    "\n",
    "## Future Work  \n",
    "Potential future improvements include:  \n",
    "- **Testing different models** to evaluate their retrieval performance.  \n",
    "- **Expanding the dataset** to enhance generalization.  \n",
    "- **Evaluating retrieval quality** using metrics such as **recall@N** and **mAP (mean Average Precision)**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b1a462a-48fc-423d-914e-d1e63292980d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (venv)",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
